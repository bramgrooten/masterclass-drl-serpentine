{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div align=\"center\">\n",
    "<a href=\"https://vbti.nl\"><img src=\"https://docs.google.com/uc?export=download&id=1DdCGllL51O5wBuiI0rwygofKx3YIDPHX\" width=\"400\"></a>\n",
    "</div>\n",
    "\n",
    "# Welcome to the DRL learning masterclass!\n",
    "\n",
    "During the two day we are going to review many concepts. Since we won't have time to explain everything in detail, we have prepared this notebook to give you a headstart. Please, run this notebook (\"Cell\" -> \"Run All\") and make sure there are no dependencies errors.\n",
    "\n",
    "In this notebook, we have included an explaination to the main interface and the functionality of  <a href=\"https://gym.openai.com/\">[Open AI Gym]</a>, [Keras](https://keras.io/) and [Tensorflow](https://www.tensorflow.org/). \n",
    "\n",
    "If you are familiar with **OpenAI Gym Interface**, **Gym Wrappers and Monitors**, **Keras Sequential API**, **Gradient Tapes** and **Tensorflow Datasets** you can freely skip this notebook."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Importing Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "pygame 2.0.0 (SDL 2.0.12, python 3.8.5)\n",
      "Hello from the pygame community. https://www.pygame.org/contribute.html\n"
     ]
    }
   ],
   "source": [
    "import gym\n",
    "import matplotlib.pyplot as plt\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import yaml\n",
    "import pygame"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# OpenAI Gym\n",
    "### Environment Class\n",
    "The main OpenAI Gym class. It encapsulates an environment with arbitrary behind-the-scenes dynamics. You can manually create an Atari environment using this class.\n",
    "\n",
    "You can manualy create an Atari environent like this:\n",
    "```python\n",
    "from gym.envs.atari import AtariEnv\n",
    "env = AtariEnv(game='pong', obs_type='image')\n",
    "```\n",
    "However, there are a lot of premade environments that are stored in a registry. This is convenient because you need to know only the *id* of the environment to create it.\n",
    "\n",
    "```python\n",
    "import gym\n",
    "env = gym.make(id='PongDeterministic-v4')\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can check the list of all available environments with the following command:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Copy-v0', 'RepeatCopy-v0', 'ReversedAddition-v0', 'ReversedAddition3-v0', 'DuplicatedInput-v0', 'Reverse-v0', 'CartPole-v0', 'CartPole-v1', 'MountainCar-v0', 'MountainCarContinuous-v0', 'Pendulum-v0', 'Acrobot-v1', 'LunarLander-v2', 'LunarLanderContinuous-v2', 'BipedalWalker-v3', 'BipedalWalkerHardcore-v3', 'CarRacing-v0', 'Blackjack-v0', 'KellyCoinflip-v0', 'KellyCoinflipGeneralized-v0', 'FrozenLake-v0', 'FrozenLake8x8-v0', 'CliffWalking-v0', 'NChain-v0', 'Roulette-v0', 'Taxi-v3', 'GuessingGame-v0', 'HotterColder-v0', 'Reacher-v2', 'Pusher-v2', 'Thrower-v2', 'Striker-v2', 'InvertedPendulum-v2', 'InvertedDoublePendulum-v2', 'HalfCheetah-v2', 'HalfCheetah-v3', 'Hopper-v2', 'Hopper-v3', 'Swimmer-v2', 'Swimmer-v3', 'Walker2d-v2', 'Walker2d-v3', 'Ant-v2', 'Ant-v3', 'Humanoid-v2', 'Humanoid-v3', 'HumanoidStandup-v2', 'FetchSlide-v1', 'FetchPickAndPlace-v1', 'FetchReach-v1', 'FetchPush-v1', 'HandReach-v0', 'HandManipulateBlockRotateZ-v0', 'HandManipulateBlockRotateZTouchSensors-v0', 'HandManipulateBlockRotateZTouchSensors-v1', 'HandManipulateBlockRotateParallel-v0', 'HandManipulateBlockRotateParallelTouchSensors-v0', 'HandManipulateBlockRotateParallelTouchSensors-v1', 'HandManipulateBlockRotateXYZ-v0', 'HandManipulateBlockRotateXYZTouchSensors-v0', 'HandManipulateBlockRotateXYZTouchSensors-v1', 'HandManipulateBlockFull-v0', 'HandManipulateBlock-v0', 'HandManipulateBlockTouchSensors-v0', 'HandManipulateBlockTouchSensors-v1', 'HandManipulateEggRotate-v0', 'HandManipulateEggRotateTouchSensors-v0', 'HandManipulateEggRotateTouchSensors-v1', 'HandManipulateEggFull-v0', 'HandManipulateEgg-v0', 'HandManipulateEggTouchSensors-v0', 'HandManipulateEggTouchSensors-v1', 'HandManipulatePenRotate-v0', 'HandManipulatePenRotateTouchSensors-v0', 'HandManipulatePenRotateTouchSensors-v1', 'HandManipulatePenFull-v0', 'HandManipulatePen-v0', 'HandManipulatePenTouchSensors-v0', 'HandManipulatePenTouchSensors-v1', 'FetchSlideDense-v1', 'FetchPickAndPlaceDense-v1', 'FetchReachDense-v1', 'FetchPushDense-v1', 'HandReachDense-v0', 'HandManipulateBlockRotateZDense-v0', 'HandManipulateBlockRotateZTouchSensorsDense-v0', 'HandManipulateBlockRotateZTouchSensorsDense-v1', 'HandManipulateBlockRotateParallelDense-v0', 'HandManipulateBlockRotateParallelTouchSensorsDense-v0', 'HandManipulateBlockRotateParallelTouchSensorsDense-v1', 'HandManipulateBlockRotateXYZDense-v0', 'HandManipulateBlockRotateXYZTouchSensorsDense-v0', 'HandManipulateBlockRotateXYZTouchSensorsDense-v1', 'HandManipulateBlockFullDense-v0', 'HandManipulateBlockDense-v0', 'HandManipulateBlockTouchSensorsDense-v0', 'HandManipulateBlockTouchSensorsDense-v1', 'HandManipulateEggRotateDense-v0', 'HandManipulateEggRotateTouchSensorsDense-v0', 'HandManipulateEggRotateTouchSensorsDense-v1', 'HandManipulateEggFullDense-v0', 'HandManipulateEggDense-v0', 'HandManipulateEggTouchSensorsDense-v0', 'HandManipulateEggTouchSensorsDense-v1', 'HandManipulatePenRotateDense-v0', 'HandManipulatePenRotateTouchSensorsDense-v0', 'HandManipulatePenRotateTouchSensorsDense-v1', 'HandManipulatePenFullDense-v0', 'HandManipulatePenDense-v0', 'HandManipulatePenTouchSensorsDense-v0', 'HandManipulatePenTouchSensorsDense-v1', 'Adventure-v0', 'Adventure-v4', 'AdventureDeterministic-v0', 'AdventureDeterministic-v4', 'AdventureNoFrameskip-v0', 'AdventureNoFrameskip-v4', 'Adventure-ram-v0', 'Adventure-ram-v4', 'Adventure-ramDeterministic-v0', 'Adventure-ramDeterministic-v4', 'Adventure-ramNoFrameskip-v0', 'Adventure-ramNoFrameskip-v4', 'AirRaid-v0', 'AirRaid-v4', 'AirRaidDeterministic-v0', 'AirRaidDeterministic-v4', 'AirRaidNoFrameskip-v0', 'AirRaidNoFrameskip-v4', 'AirRaid-ram-v0', 'AirRaid-ram-v4', 'AirRaid-ramDeterministic-v0', 'AirRaid-ramDeterministic-v4', 'AirRaid-ramNoFrameskip-v0', 'AirRaid-ramNoFrameskip-v4', 'Alien-v0', 'Alien-v4', 'AlienDeterministic-v0', 'AlienDeterministic-v4', 'AlienNoFrameskip-v0', 'AlienNoFrameskip-v4', 'Alien-ram-v0', 'Alien-ram-v4', 'Alien-ramDeterministic-v0', 'Alien-ramDeterministic-v4', 'Alien-ramNoFrameskip-v0', 'Alien-ramNoFrameskip-v4', 'Amidar-v0', 'Amidar-v4', 'AmidarDeterministic-v0', 'AmidarDeterministic-v4', 'AmidarNoFrameskip-v0', 'AmidarNoFrameskip-v4', 'Amidar-ram-v0', 'Amidar-ram-v4', 'Amidar-ramDeterministic-v0', 'Amidar-ramDeterministic-v4', 'Amidar-ramNoFrameskip-v0', 'Amidar-ramNoFrameskip-v4', 'Assault-v0', 'Assault-v4', 'AssaultDeterministic-v0', 'AssaultDeterministic-v4', 'AssaultNoFrameskip-v0', 'AssaultNoFrameskip-v4', 'Assault-ram-v0', 'Assault-ram-v4', 'Assault-ramDeterministic-v0', 'Assault-ramDeterministic-v4', 'Assault-ramNoFrameskip-v0', 'Assault-ramNoFrameskip-v4', 'Asterix-v0', 'Asterix-v4', 'AsterixDeterministic-v0', 'AsterixDeterministic-v4', 'AsterixNoFrameskip-v0', 'AsterixNoFrameskip-v4', 'Asterix-ram-v0', 'Asterix-ram-v4', 'Asterix-ramDeterministic-v0', 'Asterix-ramDeterministic-v4', 'Asterix-ramNoFrameskip-v0', 'Asterix-ramNoFrameskip-v4', 'Asteroids-v0', 'Asteroids-v4', 'AsteroidsDeterministic-v0', 'AsteroidsDeterministic-v4', 'AsteroidsNoFrameskip-v0', 'AsteroidsNoFrameskip-v4', 'Asteroids-ram-v0', 'Asteroids-ram-v4', 'Asteroids-ramDeterministic-v0', 'Asteroids-ramDeterministic-v4', 'Asteroids-ramNoFrameskip-v0', 'Asteroids-ramNoFrameskip-v4', 'Atlantis-v0', 'Atlantis-v4', 'AtlantisDeterministic-v0', 'AtlantisDeterministic-v4', 'AtlantisNoFrameskip-v0', 'AtlantisNoFrameskip-v4', 'Atlantis-ram-v0', 'Atlantis-ram-v4', 'Atlantis-ramDeterministic-v0', 'Atlantis-ramDeterministic-v4', 'Atlantis-ramNoFrameskip-v0', 'Atlantis-ramNoFrameskip-v4', 'BankHeist-v0', 'BankHeist-v4', 'BankHeistDeterministic-v0', 'BankHeistDeterministic-v4', 'BankHeistNoFrameskip-v0', 'BankHeistNoFrameskip-v4', 'BankHeist-ram-v0', 'BankHeist-ram-v4', 'BankHeist-ramDeterministic-v0', 'BankHeist-ramDeterministic-v4', 'BankHeist-ramNoFrameskip-v0', 'BankHeist-ramNoFrameskip-v4', 'BattleZone-v0', 'BattleZone-v4', 'BattleZoneDeterministic-v0', 'BattleZoneDeterministic-v4', 'BattleZoneNoFrameskip-v0', 'BattleZoneNoFrameskip-v4', 'BattleZone-ram-v0', 'BattleZone-ram-v4', 'BattleZone-ramDeterministic-v0', 'BattleZone-ramDeterministic-v4', 'BattleZone-ramNoFrameskip-v0', 'BattleZone-ramNoFrameskip-v4', 'BeamRider-v0', 'BeamRider-v4', 'BeamRiderDeterministic-v0', 'BeamRiderDeterministic-v4', 'BeamRiderNoFrameskip-v0', 'BeamRiderNoFrameskip-v4', 'BeamRider-ram-v0', 'BeamRider-ram-v4', 'BeamRider-ramDeterministic-v0', 'BeamRider-ramDeterministic-v4', 'BeamRider-ramNoFrameskip-v0', 'BeamRider-ramNoFrameskip-v4', 'Berzerk-v0', 'Berzerk-v4', 'BerzerkDeterministic-v0', 'BerzerkDeterministic-v4', 'BerzerkNoFrameskip-v0', 'BerzerkNoFrameskip-v4', 'Berzerk-ram-v0', 'Berzerk-ram-v4', 'Berzerk-ramDeterministic-v0', 'Berzerk-ramDeterministic-v4', 'Berzerk-ramNoFrameskip-v0', 'Berzerk-ramNoFrameskip-v4', 'Bowling-v0', 'Bowling-v4', 'BowlingDeterministic-v0', 'BowlingDeterministic-v4', 'BowlingNoFrameskip-v0', 'BowlingNoFrameskip-v4', 'Bowling-ram-v0', 'Bowling-ram-v4', 'Bowling-ramDeterministic-v0', 'Bowling-ramDeterministic-v4', 'Bowling-ramNoFrameskip-v0', 'Bowling-ramNoFrameskip-v4', 'Boxing-v0', 'Boxing-v4', 'BoxingDeterministic-v0', 'BoxingDeterministic-v4', 'BoxingNoFrameskip-v0', 'BoxingNoFrameskip-v4', 'Boxing-ram-v0', 'Boxing-ram-v4', 'Boxing-ramDeterministic-v0', 'Boxing-ramDeterministic-v4', 'Boxing-ramNoFrameskip-v0', 'Boxing-ramNoFrameskip-v4', 'Breakout-v0', 'Breakout-v4', 'BreakoutDeterministic-v0', 'BreakoutDeterministic-v4', 'BreakoutNoFrameskip-v0', 'BreakoutNoFrameskip-v4', 'Breakout-ram-v0', 'Breakout-ram-v4', 'Breakout-ramDeterministic-v0', 'Breakout-ramDeterministic-v4', 'Breakout-ramNoFrameskip-v0', 'Breakout-ramNoFrameskip-v4', 'Carnival-v0', 'Carnival-v4', 'CarnivalDeterministic-v0', 'CarnivalDeterministic-v4', 'CarnivalNoFrameskip-v0', 'CarnivalNoFrameskip-v4', 'Carnival-ram-v0', 'Carnival-ram-v4', 'Carnival-ramDeterministic-v0', 'Carnival-ramDeterministic-v4', 'Carnival-ramNoFrameskip-v0', 'Carnival-ramNoFrameskip-v4', 'Centipede-v0', 'Centipede-v4', 'CentipedeDeterministic-v0', 'CentipedeDeterministic-v4', 'CentipedeNoFrameskip-v0', 'CentipedeNoFrameskip-v4', 'Centipede-ram-v0', 'Centipede-ram-v4', 'Centipede-ramDeterministic-v0', 'Centipede-ramDeterministic-v4', 'Centipede-ramNoFrameskip-v0', 'Centipede-ramNoFrameskip-v4', 'ChopperCommand-v0', 'ChopperCommand-v4', 'ChopperCommandDeterministic-v0', 'ChopperCommandDeterministic-v4', 'ChopperCommandNoFrameskip-v0', 'ChopperCommandNoFrameskip-v4', 'ChopperCommand-ram-v0', 'ChopperCommand-ram-v4', 'ChopperCommand-ramDeterministic-v0', 'ChopperCommand-ramDeterministic-v4', 'ChopperCommand-ramNoFrameskip-v0', 'ChopperCommand-ramNoFrameskip-v4', 'CrazyClimber-v0', 'CrazyClimber-v4', 'CrazyClimberDeterministic-v0', 'CrazyClimberDeterministic-v4', 'CrazyClimberNoFrameskip-v0', 'CrazyClimberNoFrameskip-v4', 'CrazyClimber-ram-v0', 'CrazyClimber-ram-v4', 'CrazyClimber-ramDeterministic-v0', 'CrazyClimber-ramDeterministic-v4', 'CrazyClimber-ramNoFrameskip-v0', 'CrazyClimber-ramNoFrameskip-v4', 'Defender-v0', 'Defender-v4', 'DefenderDeterministic-v0', 'DefenderDeterministic-v4', 'DefenderNoFrameskip-v0', 'DefenderNoFrameskip-v4', 'Defender-ram-v0', 'Defender-ram-v4', 'Defender-ramDeterministic-v0', 'Defender-ramDeterministic-v4', 'Defender-ramNoFrameskip-v0', 'Defender-ramNoFrameskip-v4', 'DemonAttack-v0', 'DemonAttack-v4', 'DemonAttackDeterministic-v0', 'DemonAttackDeterministic-v4', 'DemonAttackNoFrameskip-v0', 'DemonAttackNoFrameskip-v4', 'DemonAttack-ram-v0', 'DemonAttack-ram-v4', 'DemonAttack-ramDeterministic-v0', 'DemonAttack-ramDeterministic-v4', 'DemonAttack-ramNoFrameskip-v0', 'DemonAttack-ramNoFrameskip-v4', 'DoubleDunk-v0', 'DoubleDunk-v4', 'DoubleDunkDeterministic-v0', 'DoubleDunkDeterministic-v4', 'DoubleDunkNoFrameskip-v0', 'DoubleDunkNoFrameskip-v4', 'DoubleDunk-ram-v0', 'DoubleDunk-ram-v4', 'DoubleDunk-ramDeterministic-v0', 'DoubleDunk-ramDeterministic-v4', 'DoubleDunk-ramNoFrameskip-v0', 'DoubleDunk-ramNoFrameskip-v4', 'ElevatorAction-v0', 'ElevatorAction-v4', 'ElevatorActionDeterministic-v0', 'ElevatorActionDeterministic-v4', 'ElevatorActionNoFrameskip-v0', 'ElevatorActionNoFrameskip-v4', 'ElevatorAction-ram-v0', 'ElevatorAction-ram-v4', 'ElevatorAction-ramDeterministic-v0', 'ElevatorAction-ramDeterministic-v4', 'ElevatorAction-ramNoFrameskip-v0', 'ElevatorAction-ramNoFrameskip-v4', 'Enduro-v0', 'Enduro-v4', 'EnduroDeterministic-v0', 'EnduroDeterministic-v4', 'EnduroNoFrameskip-v0', 'EnduroNoFrameskip-v4', 'Enduro-ram-v0', 'Enduro-ram-v4', 'Enduro-ramDeterministic-v0', 'Enduro-ramDeterministic-v4', 'Enduro-ramNoFrameskip-v0', 'Enduro-ramNoFrameskip-v4', 'FishingDerby-v0', 'FishingDerby-v4', 'FishingDerbyDeterministic-v0', 'FishingDerbyDeterministic-v4', 'FishingDerbyNoFrameskip-v0', 'FishingDerbyNoFrameskip-v4', 'FishingDerby-ram-v0', 'FishingDerby-ram-v4', 'FishingDerby-ramDeterministic-v0', 'FishingDerby-ramDeterministic-v4', 'FishingDerby-ramNoFrameskip-v0', 'FishingDerby-ramNoFrameskip-v4', 'Freeway-v0', 'Freeway-v4', 'FreewayDeterministic-v0', 'FreewayDeterministic-v4', 'FreewayNoFrameskip-v0', 'FreewayNoFrameskip-v4', 'Freeway-ram-v0', 'Freeway-ram-v4', 'Freeway-ramDeterministic-v0', 'Freeway-ramDeterministic-v4', 'Freeway-ramNoFrameskip-v0', 'Freeway-ramNoFrameskip-v4', 'Frostbite-v0', 'Frostbite-v4', 'FrostbiteDeterministic-v0', 'FrostbiteDeterministic-v4', 'FrostbiteNoFrameskip-v0', 'FrostbiteNoFrameskip-v4', 'Frostbite-ram-v0', 'Frostbite-ram-v4', 'Frostbite-ramDeterministic-v0', 'Frostbite-ramDeterministic-v4', 'Frostbite-ramNoFrameskip-v0', 'Frostbite-ramNoFrameskip-v4', 'Gopher-v0', 'Gopher-v4', 'GopherDeterministic-v0', 'GopherDeterministic-v4', 'GopherNoFrameskip-v0', 'GopherNoFrameskip-v4', 'Gopher-ram-v0', 'Gopher-ram-v4', 'Gopher-ramDeterministic-v0', 'Gopher-ramDeterministic-v4', 'Gopher-ramNoFrameskip-v0', 'Gopher-ramNoFrameskip-v4', 'Gravitar-v0', 'Gravitar-v4', 'GravitarDeterministic-v0', 'GravitarDeterministic-v4', 'GravitarNoFrameskip-v0', 'GravitarNoFrameskip-v4', 'Gravitar-ram-v0', 'Gravitar-ram-v4', 'Gravitar-ramDeterministic-v0', 'Gravitar-ramDeterministic-v4', 'Gravitar-ramNoFrameskip-v0', 'Gravitar-ramNoFrameskip-v4', 'Hero-v0', 'Hero-v4', 'HeroDeterministic-v0', 'HeroDeterministic-v4', 'HeroNoFrameskip-v0', 'HeroNoFrameskip-v4', 'Hero-ram-v0', 'Hero-ram-v4', 'Hero-ramDeterministic-v0', 'Hero-ramDeterministic-v4', 'Hero-ramNoFrameskip-v0', 'Hero-ramNoFrameskip-v4', 'IceHockey-v0', 'IceHockey-v4', 'IceHockeyDeterministic-v0', 'IceHockeyDeterministic-v4', 'IceHockeyNoFrameskip-v0', 'IceHockeyNoFrameskip-v4', 'IceHockey-ram-v0', 'IceHockey-ram-v4', 'IceHockey-ramDeterministic-v0', 'IceHockey-ramDeterministic-v4', 'IceHockey-ramNoFrameskip-v0', 'IceHockey-ramNoFrameskip-v4', 'Jamesbond-v0', 'Jamesbond-v4', 'JamesbondDeterministic-v0', 'JamesbondDeterministic-v4', 'JamesbondNoFrameskip-v0', 'JamesbondNoFrameskip-v4', 'Jamesbond-ram-v0', 'Jamesbond-ram-v4', 'Jamesbond-ramDeterministic-v0', 'Jamesbond-ramDeterministic-v4', 'Jamesbond-ramNoFrameskip-v0', 'Jamesbond-ramNoFrameskip-v4', 'JourneyEscape-v0', 'JourneyEscape-v4', 'JourneyEscapeDeterministic-v0', 'JourneyEscapeDeterministic-v4', 'JourneyEscapeNoFrameskip-v0', 'JourneyEscapeNoFrameskip-v4', 'JourneyEscape-ram-v0', 'JourneyEscape-ram-v4', 'JourneyEscape-ramDeterministic-v0', 'JourneyEscape-ramDeterministic-v4', 'JourneyEscape-ramNoFrameskip-v0', 'JourneyEscape-ramNoFrameskip-v4', 'Kangaroo-v0', 'Kangaroo-v4', 'KangarooDeterministic-v0', 'KangarooDeterministic-v4', 'KangarooNoFrameskip-v0', 'KangarooNoFrameskip-v4', 'Kangaroo-ram-v0', 'Kangaroo-ram-v4', 'Kangaroo-ramDeterministic-v0', 'Kangaroo-ramDeterministic-v4', 'Kangaroo-ramNoFrameskip-v0', 'Kangaroo-ramNoFrameskip-v4', 'Krull-v0', 'Krull-v4', 'KrullDeterministic-v0', 'KrullDeterministic-v4', 'KrullNoFrameskip-v0', 'KrullNoFrameskip-v4', 'Krull-ram-v0', 'Krull-ram-v4', 'Krull-ramDeterministic-v0', 'Krull-ramDeterministic-v4', 'Krull-ramNoFrameskip-v0', 'Krull-ramNoFrameskip-v4', 'KungFuMaster-v0', 'KungFuMaster-v4', 'KungFuMasterDeterministic-v0', 'KungFuMasterDeterministic-v4', 'KungFuMasterNoFrameskip-v0', 'KungFuMasterNoFrameskip-v4', 'KungFuMaster-ram-v0', 'KungFuMaster-ram-v4', 'KungFuMaster-ramDeterministic-v0', 'KungFuMaster-ramDeterministic-v4', 'KungFuMaster-ramNoFrameskip-v0', 'KungFuMaster-ramNoFrameskip-v4', 'MontezumaRevenge-v0', 'MontezumaRevenge-v4', 'MontezumaRevengeDeterministic-v0', 'MontezumaRevengeDeterministic-v4', 'MontezumaRevengeNoFrameskip-v0', 'MontezumaRevengeNoFrameskip-v4', 'MontezumaRevenge-ram-v0', 'MontezumaRevenge-ram-v4', 'MontezumaRevenge-ramDeterministic-v0', 'MontezumaRevenge-ramDeterministic-v4', 'MontezumaRevenge-ramNoFrameskip-v0', 'MontezumaRevenge-ramNoFrameskip-v4', 'MsPacman-v0', 'MsPacman-v4', 'MsPacmanDeterministic-v0', 'MsPacmanDeterministic-v4', 'MsPacmanNoFrameskip-v0', 'MsPacmanNoFrameskip-v4', 'MsPacman-ram-v0', 'MsPacman-ram-v4', 'MsPacman-ramDeterministic-v0', 'MsPacman-ramDeterministic-v4', 'MsPacman-ramNoFrameskip-v0', 'MsPacman-ramNoFrameskip-v4', 'NameThisGame-v0', 'NameThisGame-v4', 'NameThisGameDeterministic-v0', 'NameThisGameDeterministic-v4', 'NameThisGameNoFrameskip-v0', 'NameThisGameNoFrameskip-v4', 'NameThisGame-ram-v0', 'NameThisGame-ram-v4', 'NameThisGame-ramDeterministic-v0', 'NameThisGame-ramDeterministic-v4', 'NameThisGame-ramNoFrameskip-v0', 'NameThisGame-ramNoFrameskip-v4', 'Phoenix-v0', 'Phoenix-v4', 'PhoenixDeterministic-v0', 'PhoenixDeterministic-v4', 'PhoenixNoFrameskip-v0', 'PhoenixNoFrameskip-v4', 'Phoenix-ram-v0', 'Phoenix-ram-v4', 'Phoenix-ramDeterministic-v0', 'Phoenix-ramDeterministic-v4', 'Phoenix-ramNoFrameskip-v0', 'Phoenix-ramNoFrameskip-v4', 'Pitfall-v0', 'Pitfall-v4', 'PitfallDeterministic-v0', 'PitfallDeterministic-v4', 'PitfallNoFrameskip-v0', 'PitfallNoFrameskip-v4', 'Pitfall-ram-v0', 'Pitfall-ram-v4', 'Pitfall-ramDeterministic-v0', 'Pitfall-ramDeterministic-v4', 'Pitfall-ramNoFrameskip-v0', 'Pitfall-ramNoFrameskip-v4', 'Pong-v0', 'Pong-v4', 'PongDeterministic-v0', 'PongDeterministic-v4', 'PongNoFrameskip-v0', 'PongNoFrameskip-v4', 'Pong-ram-v0', 'Pong-ram-v4', 'Pong-ramDeterministic-v0', 'Pong-ramDeterministic-v4', 'Pong-ramNoFrameskip-v0', 'Pong-ramNoFrameskip-v4', 'Pooyan-v0', 'Pooyan-v4', 'PooyanDeterministic-v0', 'PooyanDeterministic-v4', 'PooyanNoFrameskip-v0', 'PooyanNoFrameskip-v4', 'Pooyan-ram-v0', 'Pooyan-ram-v4', 'Pooyan-ramDeterministic-v0', 'Pooyan-ramDeterministic-v4', 'Pooyan-ramNoFrameskip-v0', 'Pooyan-ramNoFrameskip-v4', 'PrivateEye-v0', 'PrivateEye-v4', 'PrivateEyeDeterministic-v0', 'PrivateEyeDeterministic-v4', 'PrivateEyeNoFrameskip-v0', 'PrivateEyeNoFrameskip-v4', 'PrivateEye-ram-v0', 'PrivateEye-ram-v4', 'PrivateEye-ramDeterministic-v0', 'PrivateEye-ramDeterministic-v4', 'PrivateEye-ramNoFrameskip-v0', 'PrivateEye-ramNoFrameskip-v4', 'Qbert-v0', 'Qbert-v4', 'QbertDeterministic-v0', 'QbertDeterministic-v4', 'QbertNoFrameskip-v0', 'QbertNoFrameskip-v4', 'Qbert-ram-v0', 'Qbert-ram-v4', 'Qbert-ramDeterministic-v0', 'Qbert-ramDeterministic-v4', 'Qbert-ramNoFrameskip-v0', 'Qbert-ramNoFrameskip-v4', 'Riverraid-v0', 'Riverraid-v4', 'RiverraidDeterministic-v0', 'RiverraidDeterministic-v4', 'RiverraidNoFrameskip-v0', 'RiverraidNoFrameskip-v4', 'Riverraid-ram-v0', 'Riverraid-ram-v4', 'Riverraid-ramDeterministic-v0', 'Riverraid-ramDeterministic-v4', 'Riverraid-ramNoFrameskip-v0', 'Riverraid-ramNoFrameskip-v4', 'RoadRunner-v0', 'RoadRunner-v4', 'RoadRunnerDeterministic-v0', 'RoadRunnerDeterministic-v4', 'RoadRunnerNoFrameskip-v0', 'RoadRunnerNoFrameskip-v4', 'RoadRunner-ram-v0', 'RoadRunner-ram-v4', 'RoadRunner-ramDeterministic-v0', 'RoadRunner-ramDeterministic-v4', 'RoadRunner-ramNoFrameskip-v0', 'RoadRunner-ramNoFrameskip-v4', 'Robotank-v0', 'Robotank-v4', 'RobotankDeterministic-v0', 'RobotankDeterministic-v4', 'RobotankNoFrameskip-v0', 'RobotankNoFrameskip-v4', 'Robotank-ram-v0', 'Robotank-ram-v4', 'Robotank-ramDeterministic-v0', 'Robotank-ramDeterministic-v4', 'Robotank-ramNoFrameskip-v0', 'Robotank-ramNoFrameskip-v4', 'Seaquest-v0', 'Seaquest-v4', 'SeaquestDeterministic-v0', 'SeaquestDeterministic-v4', 'SeaquestNoFrameskip-v0', 'SeaquestNoFrameskip-v4', 'Seaquest-ram-v0', 'Seaquest-ram-v4', 'Seaquest-ramDeterministic-v0', 'Seaquest-ramDeterministic-v4', 'Seaquest-ramNoFrameskip-v0', 'Seaquest-ramNoFrameskip-v4', 'Skiing-v0', 'Skiing-v4', 'SkiingDeterministic-v0', 'SkiingDeterministic-v4', 'SkiingNoFrameskip-v0', 'SkiingNoFrameskip-v4', 'Skiing-ram-v0', 'Skiing-ram-v4', 'Skiing-ramDeterministic-v0', 'Skiing-ramDeterministic-v4', 'Skiing-ramNoFrameskip-v0', 'Skiing-ramNoFrameskip-v4', 'Solaris-v0', 'Solaris-v4', 'SolarisDeterministic-v0', 'SolarisDeterministic-v4', 'SolarisNoFrameskip-v0', 'SolarisNoFrameskip-v4', 'Solaris-ram-v0', 'Solaris-ram-v4', 'Solaris-ramDeterministic-v0', 'Solaris-ramDeterministic-v4', 'Solaris-ramNoFrameskip-v0', 'Solaris-ramNoFrameskip-v4', 'SpaceInvaders-v0', 'SpaceInvaders-v4', 'SpaceInvadersDeterministic-v0', 'SpaceInvadersDeterministic-v4', 'SpaceInvadersNoFrameskip-v0', 'SpaceInvadersNoFrameskip-v4', 'SpaceInvaders-ram-v0', 'SpaceInvaders-ram-v4', 'SpaceInvaders-ramDeterministic-v0', 'SpaceInvaders-ramDeterministic-v4', 'SpaceInvaders-ramNoFrameskip-v0', 'SpaceInvaders-ramNoFrameskip-v4', 'StarGunner-v0', 'StarGunner-v4', 'StarGunnerDeterministic-v0', 'StarGunnerDeterministic-v4', 'StarGunnerNoFrameskip-v0', 'StarGunnerNoFrameskip-v4', 'StarGunner-ram-v0', 'StarGunner-ram-v4', 'StarGunner-ramDeterministic-v0', 'StarGunner-ramDeterministic-v4', 'StarGunner-ramNoFrameskip-v0', 'StarGunner-ramNoFrameskip-v4', 'Tennis-v0', 'Tennis-v4', 'TennisDeterministic-v0', 'TennisDeterministic-v4', 'TennisNoFrameskip-v0', 'TennisNoFrameskip-v4', 'Tennis-ram-v0', 'Tennis-ram-v4', 'Tennis-ramDeterministic-v0', 'Tennis-ramDeterministic-v4', 'Tennis-ramNoFrameskip-v0', 'Tennis-ramNoFrameskip-v4', 'TimePilot-v0', 'TimePilot-v4', 'TimePilotDeterministic-v0', 'TimePilotDeterministic-v4', 'TimePilotNoFrameskip-v0', 'TimePilotNoFrameskip-v4', 'TimePilot-ram-v0', 'TimePilot-ram-v4', 'TimePilot-ramDeterministic-v0', 'TimePilot-ramDeterministic-v4', 'TimePilot-ramNoFrameskip-v0', 'TimePilot-ramNoFrameskip-v4', 'Tutankham-v0', 'Tutankham-v4', 'TutankhamDeterministic-v0', 'TutankhamDeterministic-v4', 'TutankhamNoFrameskip-v0', 'TutankhamNoFrameskip-v4', 'Tutankham-ram-v0', 'Tutankham-ram-v4', 'Tutankham-ramDeterministic-v0', 'Tutankham-ramDeterministic-v4', 'Tutankham-ramNoFrameskip-v0', 'Tutankham-ramNoFrameskip-v4', 'UpNDown-v0', 'UpNDown-v4', 'UpNDownDeterministic-v0', 'UpNDownDeterministic-v4', 'UpNDownNoFrameskip-v0', 'UpNDownNoFrameskip-v4', 'UpNDown-ram-v0', 'UpNDown-ram-v4', 'UpNDown-ramDeterministic-v0', 'UpNDown-ramDeterministic-v4', 'UpNDown-ramNoFrameskip-v0', 'UpNDown-ramNoFrameskip-v4', 'Venture-v0', 'Venture-v4', 'VentureDeterministic-v0', 'VentureDeterministic-v4', 'VentureNoFrameskip-v0', 'VentureNoFrameskip-v4', 'Venture-ram-v0', 'Venture-ram-v4', 'Venture-ramDeterministic-v0', 'Venture-ramDeterministic-v4', 'Venture-ramNoFrameskip-v0', 'Venture-ramNoFrameskip-v4', 'VideoPinball-v0', 'VideoPinball-v4', 'VideoPinballDeterministic-v0', 'VideoPinballDeterministic-v4', 'VideoPinballNoFrameskip-v0', 'VideoPinballNoFrameskip-v4', 'VideoPinball-ram-v0', 'VideoPinball-ram-v4', 'VideoPinball-ramDeterministic-v0', 'VideoPinball-ramDeterministic-v4', 'VideoPinball-ramNoFrameskip-v0', 'VideoPinball-ramNoFrameskip-v4', 'WizardOfWor-v0', 'WizardOfWor-v4', 'WizardOfWorDeterministic-v0', 'WizardOfWorDeterministic-v4', 'WizardOfWorNoFrameskip-v0', 'WizardOfWorNoFrameskip-v4', 'WizardOfWor-ram-v0', 'WizardOfWor-ram-v4', 'WizardOfWor-ramDeterministic-v0', 'WizardOfWor-ramDeterministic-v4', 'WizardOfWor-ramNoFrameskip-v0', 'WizardOfWor-ramNoFrameskip-v4', 'YarsRevenge-v0', 'YarsRevenge-v4', 'YarsRevengeDeterministic-v0', 'YarsRevengeDeterministic-v4', 'YarsRevengeNoFrameskip-v0', 'YarsRevengeNoFrameskip-v4', 'YarsRevenge-ram-v0', 'YarsRevenge-ram-v4', 'YarsRevenge-ramDeterministic-v0', 'YarsRevenge-ramDeterministic-v4', 'YarsRevenge-ramNoFrameskip-v0', 'YarsRevenge-ramNoFrameskip-v4', 'Zaxxon-v0', 'Zaxxon-v4', 'ZaxxonDeterministic-v0', 'ZaxxonDeterministic-v4', 'ZaxxonNoFrameskip-v0', 'ZaxxonNoFrameskip-v4', 'Zaxxon-ram-v0', 'Zaxxon-ram-v4', 'Zaxxon-ramDeterministic-v0', 'Zaxxon-ramDeterministic-v4', 'Zaxxon-ramNoFrameskip-v0', 'Zaxxon-ramNoFrameskip-v4', 'CubeCrash-v0', 'CubeCrashSparse-v0', 'CubeCrashScreenBecomesBlack-v0', 'MemorizeDigits-v0']\n"
     ]
    }
   ],
   "source": [
    "from gym import envs\n",
    "all_envs = envs.registry.all()\n",
    "env_ids = [env_spec.id for env_spec in all_envs]\n",
    "print(env_ids)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Interface\n",
    "This class implements three usefull methods:\n",
    "- ```env.reset()``` - resets the state of the environment and returns an initial observation.\n",
    "- ```env.step(action)``` - run one timestep of the environment's dynamics. When end of episode is reached, you are responsible for calling ```reset()``` to reset this environment's state.\n",
    "- ```env.render()``` - renders the environment. Usefull if we would like to observe the agent playing.\n",
    "- ```env.observation_space``` - observation space of the environment\n",
    "- ```env.action_space``` - action space of the environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Observation space type: <class 'gym.spaces.box.Box'>\n",
      "Observation space type: <class 'gym.spaces.discrete.Discrete'>\n"
     ]
    }
   ],
   "source": [
    "import gym\n",
    "\n",
    "env = gym.make('CartPole-v1')\n",
    "print('Observation space type:', type(env.observation_space))\n",
    "print('Observation space type:', type(env.action_space))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are two major types of observation and action spaces: **Box** and **Discrete**. You can find these objects under ```gym.spaces.box.Box``` and ```gym.spaces.discrete.Discrete```."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Box Space\n",
    "**Box** space is continuous space. You can better think about it as a numpy array. It has a shape and for each element the lowest and highest possible value is defined."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Box space: Box(-3.4028234663852886e+38, 3.4028234663852886e+38, (4,), float32)\n",
      "Observation shape: (4,)\n",
      "Low: [-4.8000002e+00 -3.4028235e+38 -4.1887903e-01 -3.4028235e+38]\n",
      "High: [4.8000002e+00 3.4028235e+38 4.1887903e-01 3.4028235e+38]\n"
     ]
    }
   ],
   "source": [
    "print('Box space:', env.observation_space)\n",
    "print('Observation shape:', env.observation_space.shape)\n",
    "print('Low:', env.observation_space.low)\n",
    "print('High:', env.observation_space.high)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Discrete Space\n",
    "**Discrete** space is ... discrete. It is fully defined by the **number of actions**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Discrete space: Discrete(2)\n",
      "Number of actions: 2\n"
     ]
    }
   ],
   "source": [
    "print('Discrete space:', env.action_space)\n",
    "print('Number of actions:', env.action_space.n)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "These spaces are suitable for both **observation** and **action** spaces. There are more spaces under ```gym.spaces```, but we will not need them."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ```env.reset()```\n",
    "This method should be called **before every episode**. It will return an initial observation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[-0.03447554 -0.04520057  0.02812     0.00413918]\n"
     ]
    }
   ],
   "source": [
    "obs = env.reset()\n",
    "print(obs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ```env.step(action)```\n",
    "This method is responsible for controlling the environment. ```action``` parameter should comply with the action space of the environment. When correct action is passed it will return a ```tuple``` with 4 elements: ```next_obs, reward, done, info```.\n",
    "\n",
    "- ```next_obs``` - next observation for our agent. Unless state otherwise it has type ```np.float64```.\n",
    "- ```reward``` - scalar reward\n",
    "- ```done``` - boolean value whether the episode is finished or not. After we receive ```done==True``` we need to call ```env.reset()```. Otherwise, the behaviour of the environment will be *undefined*\n",
    "- ```info``` - dictionary with extra values returned by an environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Next obs: [-0.03537955 -0.24071428  0.02820278  0.30555999]\n",
      "Reward: 1.0\n",
      "Done: False\n",
      "Info: {}\n"
     ]
    }
   ],
   "source": [
    "action = np.random.randint(env.action_space.n)\n",
    "next_obs, reward, done, info = env.step(action)\n",
    "print('Next obs:', next_obs)\n",
    "print('Reward:', reward)\n",
    "print('Done:', done)\n",
    "print('Info:', info)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The minimal code for a loop that plays one episode should look something like this:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Steps: 41\n"
     ]
    }
   ],
   "source": [
    "done = False\n",
    "steps = 0\n",
    "env.reset()\n",
    "while not done:\n",
    "    action = env.action_space.sample()                  \n",
    "    observation, reward, done, info = env.step(action)  \n",
    "    steps += 1 \n",
    "\n",
    "print('Steps:', steps)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Atari Environments\n",
    "In the previous examples, the environments' input state was a low dimensional vector. What if we don't have the access to the internal information and can only observe the game screen? Arcade Learning Environment is the library that implements many games from the Atari 2060 and allows us to control them through a convenient Gym interface.\n",
    "\n",
    "Arcade Learning Environment ALE <a href=\"https://arxiv.org/pdf/1207.4708.pdf\"> [paper]</a> was developed in 2013 as an evaluation benchmark for general learning algorithms. Open AI gym <a href=\"https://arxiv.org/abs/1606.01540\">[paper]</a>\n",
    "\n",
    "For the Atari games the environments *ids* are following following naming convention:\n",
    "<img src=\"images/env_ids.png\" width=700>\n",
    "<center>Explanation of the naming convention. <a href=\"https://www.endtoend.ai/envs/gym/atari/\">[source]</a><center>\n",
    "\n",
    "    \n",
    "The Atari emulator generates frames at 60Hz. It would be 'unfair' if an algorithm could update its actions every frame, as a human cannot control a joystick at 60Hz. Therefore, some environments apply the same action to multiple frames. This can be a fixed number of frames or a random number of frames. The figure above explains in detail the naming convention.\n",
    "    \n",
    "Since all Atari games are inherently deterministic the brute search is a viable option for solving these games. Repeat action probability *p* is responsible for injecting stochasticity into the environment by holding the action constant for the next frame. This mechanic was proposed in Revisiting the Arcade Learning Environment <a href=\"https://arxiv.org/abs/1709.06009\">[paper]</a>. By making the environment stochastic we make sure that the agent does not rely on memorization when playing the game.\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Gym Wrappers and Monitors\n",
    "\n",
    "\n",
    "### Wrappers\n",
    "\n",
    "Environment wrapper is a very important tool that allows to extend the functionality of an environemnt in various ways. It can be used to preprocess observations, modify actions or rewards. Wrappers allows to make all of those changes in a very clean way without altering the functionality of an agent.\n",
    "\n",
    "```python\n",
    "class BasicWrapper(gym.Wrapper):\n",
    "    def __init__(self, env):\n",
    "        super().__init__(env)\n",
    "        self.env = env\n",
    "        \n",
    "    def step(self, action):\n",
    "        next_state, reward, done, info = self.env.step(action)\n",
    "        # modify ...\n",
    "        return next_state, reward, done, info\n",
    "```\n",
    "<img src=\"https://i.ibb.co/ZLfyqxt/env-wrapper.png\">\n",
    "\n",
    "## Monitors\n",
    "Monitors is a convenient way of keeping track of what the environment is doing. The most common use of the Monitor is to record a video of an agent playing. This can be done with a simple:\n",
    "\n",
    "```python\n",
    "import gym\n",
    "from gym import wrappers\n",
    "\n",
    "env = gym.wrappers.Monitor(env, directory='recording')\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Example: Random Agent\n",
    "Lets implement a random agent class that have only one ```compute_action()``` method."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "class RandomAgent:\n",
    "\n",
    "    def __init__(self, n_actions):\n",
    "        self.n_actions = n_actions\n",
    "\n",
    "    def compute_action(self, state):\n",
    "        return np.random.randint(self.n_actions)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You do not need much code to run an episode, but it would be nice to keep track the performance of the agent. Since the reward at every time step is the difference between the scores, summing up all of the rewards would give us the final score of the game."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "import matplotlib.pyplot as plt\n",
    "from IPython.core import display\n",
    "\n",
    "# Define loop for one episode\n",
    "def run_episode(env, agent, max_steps=100_000, render=False, sleep=0.):\n",
    "    is_terminal = False\n",
    "    score = 0\n",
    "    steps = 0\n",
    "    start_timer = time.time()\n",
    "\n",
    "    current_state = env.reset()\n",
    "\n",
    "    if render:\n",
    "        img = plt.imshow(env.render(mode='rgb_array'))  # only call this once\n",
    "\n",
    "    while (not is_terminal) and (steps < max_steps):\n",
    "        # get action from the agent\n",
    "\n",
    "        action = agent.compute_action(current_state)\n",
    "\n",
    "        # advance one step in the environment\n",
    "        next_state, reward, is_terminal, info = env.step(action)\n",
    "\n",
    "        # update score\n",
    "        score += reward\n",
    "\n",
    "        # update current state\n",
    "        current_state = next_state\n",
    "\n",
    "        if render:\n",
    "            # render the frame and pause\n",
    "            img.set_data(env.render(mode='rgb_array'))  # just update the data\n",
    "            display.display(plt.gcf())\n",
    "            display.clear_output(wait=True)\n",
    "            time.sleep(sleep)\n",
    "\n",
    "        steps += 1\n",
    "\n",
    "    total_time = time.time() - start_timer\n",
    "    return {'score': score,\n",
    "            'steps_per_game': steps,\n",
    "            'framerate': steps / (total_time + 1e-6)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import gym\n",
    "env = gym.make('PongDeterministic-v4')\n",
    "agent = RandomAgent(env.action_space.n)\n",
    "run_episode(env, agent, render=True, max_steps=100)\n",
    "env.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As you might notice this takes a long time to render an environement in this way. That is why we have limited the number of steps to 100. Now we are going to use a ```Monitor```."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'score': -20.0, 'steps_per_game': 838, 'framerate': 430.1238962831677}\n"
     ]
    }
   ],
   "source": [
    "from os.path import join\n",
    "video_folder = join('Experiments', 'random_pong')\n",
    "env = gym.make('PongDeterministic-v4')\n",
    "env = gym.wrappers.Monitor(env, video_folder, force=True, video_callable=lambda eps_id: True)\n",
    "\n",
    "agent = RandomAgent(env.action_space.n)\n",
    "\n",
    "stats = run_episode(env, agent, render=False)\n",
    "\n",
    "# You need to call this method for the video to be saved\n",
    "env.close()\n",
    "\n",
    "print(stats)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We have successfully recorded the gameplay of our RandomAgent. We can load the *last* recoded video with the following code."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<video src=\"Experiments\\random_pong\\openaigym.video.0.20444.video000000.mp4\" controls  >\n",
       "      Your browser does not support the <code>video</code> element.\n",
       "    </video>"
      ],
      "text/plain": [
       "<IPython.core.display.Video object>"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from IPython.display import Video\n",
    "import os\n",
    "\n",
    "# take the last element in the folder with .mp4 extension\n",
    "video_name = [x for x in os.listdir(video_folder) if '.mp4' in x][-1]\n",
    "Video(os.path.join(video_folder, video_name))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can also just navigate to the folder and open it with your regular video player."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The same piece of code works for the Cartpole or any other environment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\ikayn\\anaconda3\\envs\\rl_env\\lib\\site-packages\\pyglet\\image\\codecs\\wic.py:289: UserWarning: [WinError -2147417850] Cannot change thread mode after it is set\n",
      "  warnings.warn(str(err))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'score': 29.0, 'steps_per_game': 29, 'framerate': 24.970025278656188}\n"
     ]
    }
   ],
   "source": [
    "from os.path import join\n",
    "video_folder = join('Experiments', 'random_cartpole')\n",
    "env = gym.make('CartPole-v1')\n",
    "env = gym.wrappers.Monitor(env, video_folder, force=True, video_callable=lambda eps_id: True)\n",
    "\n",
    "agent = RandomAgent(env.action_space.n)\n",
    "\n",
    "stats = run_episode(env, agent, render=False)\n",
    "env.close()\n",
    "\n",
    "print(stats)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<video src=\"Experiments\\random_cartpole\\openaigym.video.1.20444.video000000.mp4\" controls  >\n",
       "      Your browser does not support the <code>video</code> element.\n",
       "    </video>"
      ],
      "text/plain": [
       "<IPython.core.display.Video object>"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from IPython.display import Video\n",
    "import os\n",
    "\n",
    "# take the last element in the folder with .mp4 extension\n",
    "video_name = [x for x in os.listdir(video_folder) if '.mp4' in x][-1]\n",
    "Video(os.path.join(video_folder, video_name))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training a Neural Network\n",
    "Since a big part of training Deep Reinforcement Learning agents concerns training neural networks we include a small example. In this section we are going to show you how to build and train a simple neural network using keras. We are going to use a \"Hello World\" MNIST dataset.\n",
    "\n",
    "## Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((60000, 28, 28),\n",
       " (60000,),\n",
       " (10000, 28, 28),\n",
       " (10000,),\n",
       " dtype('float64'),\n",
       " dtype('uint8'))"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from tensorflow.keras.datasets import mnist\n",
    "\n",
    "# load data\n",
    "(x_train, y_train), (x_val, y_val) = mnist.load_data()\n",
    "\n",
    "# scale data\n",
    "x_train, x_val = x_train / 255.0, x_val / 255.0\n",
    "\n",
    "# inspect shape and type\n",
    "x_train.shape, y_train.shape, x_val.shape, y_val.shape, x_val.dtype, y_train.dtype"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.image.AxesImage at 0x23d5438c550>"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# plot a sample point\n",
    "plt.imshow(x_train[36000], cmap='binary')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Change X array from shape (60000, 28, 28) ... \n",
      "to shape (60000, 784)\n",
      "Before y_train[0] = 5\n",
      "After y_train[0] = [0. 0. 0. 0. 0. 1. 0. 0. 0. 0.]\n"
     ]
    }
   ],
   "source": [
    "# reformat data in the form (batch_size, input_vector_size)\n",
    "print(\"Change X array from shape {} ... \".format(x_train.shape))\n",
    "x_train = x_train.reshape(x_train.shape[0], x_train.shape[1]*x_train.shape[2])\n",
    "x_val = x_val.reshape(x_val.shape[0], x_val.shape[1]*x_val.shape[2])\n",
    "print(\"to shape {}\".format(x_train.shape))\n",
    "\n",
    "# reformat target vector from categorical label to one-hot-encoding\n",
    "from tensorflow.keras import utils\n",
    "\n",
    "# reformat labels to one-hot-encoded labels\n",
    "print('Before y_train[0] = {}'.format(y_train[0]))\n",
    "y_train = utils.to_categorical(y_train, 10)\n",
    "y_val = utils.to_categorical(y_val, 10)\n",
    "print('After y_train[0] = {}'.format(y_train[0]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model\n",
    "Keras allows you to specify a neural network as a collection of layers. [Guide to Sequential API](https://keras.io/guides/sequential_model/)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras import models\n",
    "from tensorflow.keras import layers\n",
    "from tensorflow.keras import optimizers\n",
    "\n",
    "# define model topology\n",
    "model = models.Sequential()\n",
    "model.add(layers.Dense(40, activation='relu', input_shape=(784,)))\n",
    "model.add(layers.Dense(20, activation='relu'))\n",
    "model.add(layers.Dense(10, activation='softmax'))\n",
    "\n",
    "# define model optimization method\n",
    "model.compile(optimizer=optimizers.Adam(lr=0.001), \n",
    "              loss='binary_crossentropy', \n",
    "              metrics=['categorical_accuracy'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training a model\n",
    "## ```model.fit()```\n",
    "We can train a model by calling ```model.fit()```. This is the easiest way of training a model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20\n",
      "1000/1000 [==============================] - 1s 766us/step - loss: 0.0639 - categorical_accuracy: 0.8872 - val_loss: 0.0366 - val_categorical_accuracy: 0.9386\n",
      "Epoch 2/20\n",
      "1000/1000 [==============================] - 1s 676us/step - loss: 0.0313 - categorical_accuracy: 0.9462 - val_loss: 0.0288 - val_categorical_accuracy: 0.9504\n",
      "Epoch 3/20\n",
      "1000/1000 [==============================] - 1s 692us/step - loss: 0.0248 - categorical_accuracy: 0.9580 - val_loss: 0.0266 - val_categorical_accuracy: 0.9547\n",
      "Epoch 4/20\n",
      "1000/1000 [==============================] - 1s 683us/step - loss: 0.0207 - categorical_accuracy: 0.9649 - val_loss: 0.0221 - val_categorical_accuracy: 0.9626\n",
      "Epoch 5/20\n",
      "1000/1000 [==============================] - 1s 710us/step - loss: 0.0180 - categorical_accuracy: 0.9696 - val_loss: 0.0216 - val_categorical_accuracy: 0.9638\n",
      "Epoch 6/20\n",
      "1000/1000 [==============================] - 1s 686us/step - loss: 0.0157 - categorical_accuracy: 0.9735 - val_loss: 0.0197 - val_categorical_accuracy: 0.9676\n",
      "Epoch 7/20\n",
      "1000/1000 [==============================] - 1s 685us/step - loss: 0.0140 - categorical_accuracy: 0.9764 - val_loss: 0.0189 - val_categorical_accuracy: 0.9682\n",
      "Epoch 8/20\n",
      "1000/1000 [==============================] - 1s 663us/step - loss: 0.0125 - categorical_accuracy: 0.9789 - val_loss: 0.0187 - val_categorical_accuracy: 0.9705\n",
      "Epoch 9/20\n",
      "1000/1000 [==============================] - 1s 633us/step - loss: 0.0114 - categorical_accuracy: 0.9814 - val_loss: 0.0187 - val_categorical_accuracy: 0.9694\n",
      "Epoch 10/20\n",
      "1000/1000 [==============================] - 1s 623us/step - loss: 0.0105 - categorical_accuracy: 0.9828 - val_loss: 0.0187 - val_categorical_accuracy: 0.9696\n",
      "Epoch 11/20\n",
      "1000/1000 [==============================] - 1s 630us/step - loss: 0.0097 - categorical_accuracy: 0.9838 - val_loss: 0.0203 - val_categorical_accuracy: 0.9680\n",
      "Epoch 12/20\n",
      "1000/1000 [==============================] - 1s 631us/step - loss: 0.0089 - categorical_accuracy: 0.9849 - val_loss: 0.0183 - val_categorical_accuracy: 0.9716\n",
      "Epoch 13/20\n",
      "1000/1000 [==============================] - 1s 627us/step - loss: 0.0082 - categorical_accuracy: 0.9865 - val_loss: 0.0177 - val_categorical_accuracy: 0.9718\n",
      "Epoch 14/20\n",
      "1000/1000 [==============================] - 1s 624us/step - loss: 0.0075 - categorical_accuracy: 0.9875 - val_loss: 0.0198 - val_categorical_accuracy: 0.9685\n",
      "Epoch 15/20\n",
      "1000/1000 [==============================] - 1s 635us/step - loss: 0.0071 - categorical_accuracy: 0.9883 - val_loss: 0.0197 - val_categorical_accuracy: 0.9714\n",
      "Epoch 16/20\n",
      "1000/1000 [==============================] - 1s 632us/step - loss: 0.0066 - categorical_accuracy: 0.9897 - val_loss: 0.0181 - val_categorical_accuracy: 0.9724\n",
      "Epoch 17/20\n",
      "1000/1000 [==============================] - 1s 633us/step - loss: 0.0060 - categorical_accuracy: 0.9898 - val_loss: 0.0206 - val_categorical_accuracy: 0.9713\n",
      "Epoch 18/20\n",
      "1000/1000 [==============================] - 1s 633us/step - loss: 0.0055 - categorical_accuracy: 0.9911 - val_loss: 0.0208 - val_categorical_accuracy: 0.9702\n",
      "Epoch 19/20\n",
      "1000/1000 [==============================] - 1s 695us/step - loss: 0.0054 - categorical_accuracy: 0.9909 - val_loss: 0.0206 - val_categorical_accuracy: 0.9704\n",
      "Epoch 20/20\n",
      "1000/1000 [==============================] - 1s 640us/step - loss: 0.0052 - categorical_accuracy: 0.9912 - val_loss: 0.0215 - val_categorical_accuracy: 0.9697\n"
     ]
    }
   ],
   "source": [
    "history = model.fit(x_train, y_train, epochs=20, batch_size=60, validation_data=(x_val, y_val))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Custom training loop\n",
    "For Reinforcement Learning applicaitons it is important to know how to train a model with a custom training loop. It gives you more freedom in defining the computational flow. Make sure that you understand how all of these elements work, since we are going to use them for training our DRL agents.\n",
    "\n",
    "### ```Dataset```\n",
    "Important part of the custom training loop is the ```Dataset``` object. It allows to create, shuffle, batch and then iterate through the date in a very straightforward manner."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a dataset instance, shuffle it and set the batch size\n",
    "train_ds = tf.data.Dataset.from_tensor_slices(\n",
    "    (x_train, y_train)).shuffle(1).batch(60)\n",
    "\n",
    "# The same goes for the testing dataset\n",
    "test_ds = tf.data.Dataset.from_tensor_slices((x_val, y_val)).batch(60)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ```Losses``` and ```Metrics```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define the loss object. It can be any function that accepts\n",
    "# loss(y_true, y_predicted) and returns a scalar value\n",
    "loss_object = tf.keras.losses.binary_crossentropy\n",
    "\n",
    "# define metrics for training\n",
    "train_loss = tf.keras.metrics.Mean(name='train_loss')\n",
    "train_accuracy = tf.keras.metrics.BinaryAccuracy(name='train_accuracy')\n",
    "\n",
    "# define metrics for evaluation\n",
    "test_loss = tf.keras.metrics.Mean(name='test_loss')\n",
    "test_accuracy = tf.keras.metrics.BinaryAccuracy(name='test_accuracy')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ```GradientTape```\n",
    "Now we need to define the function were we are going to take the gradients and perform backpropagation.\n",
    "\n",
    "To take the gradients of the loss with respect to any *tensorflow* variables we can use ```tf.GradientTape()```. If you are not familiar with it you can read more [here](https://medium.com/analytics-vidhya/tf-gradienttape-explained-for-keras-users-cc3f06276f22). \n",
    "\n",
    "```GradientTape``` is a **context manager** object. You can find ore information on **context managers** [here](https://medium.com/better-programming/context-managers-in-python-go-beyond-with-open-as-file-85a27e392114).\n",
    "\n",
    "### Optimizers\n",
    "Afterwards, we will use an **optimizer** to perform a gradient descent. More information on **optimizers** can be found [here](https://analyticsindiamag.com/guide-to-optimizers-for-machine-learning/)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define the optimizer that will perform the backprobagation steps\n",
    "optimizer = tf.keras.optimizers.SGD()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ```tf.function```\n",
    "In order to make the computations more efficient we can pre-compile Python methods as a part of tensorflow computational graph using **```tf.function```** decorator. More info can be found [here](https://www.tensorflow.org/guide/function)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "@tf.function\n",
    "def train_step(images, labels):\n",
    "    with tf.GradientTape() as tape:\n",
    "        predictions = model(images)\n",
    "        loss = loss_object(labels, predictions)\n",
    "    # take the gradients with respect to all the variables\n",
    "    # that were used inside the context manager above\n",
    "    gradients = tape.gradient(loss, model.trainable_variables)\n",
    "    \n",
    "    # pass the gradients and corresponding variables to optimizer\n",
    "    optimizer.apply_gradients(zip(gradients, model.trainable_variables))\n",
    "    \n",
    "    # record loss and accuracy metric\n",
    "    train_loss(loss)\n",
    "    train_accuracy(labels, predictions)\n",
    "\n",
    "# the same goes for the test step\n",
    "@tf.function\n",
    "def test_step(features, labels):\n",
    "    predictions = model(features, training=False)\n",
    "    t_loss = loss_object(labels, predictions)\n",
    "\n",
    "    test_loss(t_loss)\n",
    "    test_accuracy(labels, predictions)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we just need to iterate through our datasets and call ```train_step()``` method."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1, Loss: 0.00128, Accuracy: 99.98, Test Loss: 0.03, Test Accuracy: 99.40\n",
      "Epoch 2, Loss: 0.00123, Accuracy: 99.98, Test Loss: 0.03, Test Accuracy: 99.40\n",
      "Epoch 3, Loss: 0.00149, Accuracy: 99.97, Test Loss: 0.03, Test Accuracy: 99.37\n",
      "Epoch 4, Loss: 0.00131, Accuracy: 99.97, Test Loss: 0.03, Test Accuracy: 99.40\n",
      "Epoch 5, Loss: 0.00113, Accuracy: 99.98, Test Loss: 0.03, Test Accuracy: 99.39\n",
      "Epoch 6, Loss: 0.00141, Accuracy: 99.97, Test Loss: 0.03, Test Accuracy: 99.40\n",
      "Epoch 7, Loss: 0.00242, Accuracy: 99.93, Test Loss: 0.03, Test Accuracy: 99.38\n",
      "Epoch 8, Loss: 0.00155, Accuracy: 99.95, Test Loss: 0.03, Test Accuracy: 99.31\n",
      "Epoch 9, Loss: 0.00214, Accuracy: 99.93, Test Loss: 0.03, Test Accuracy: 99.40\n",
      "Epoch 10, Loss: 0.00211, Accuracy: 99.93, Test Loss: 0.03, Test Accuracy: 99.36\n"
     ]
    }
   ],
   "source": [
    "# define model topology\n",
    "model = models.Sequential()\n",
    "model.add(layers.Dense(40, activation='relu', input_shape=(784,)))\n",
    "model.add(layers.Dense(20, activation='relu'))\n",
    "model.add(layers.Dense(10, activation='softmax'))\n",
    "\n",
    "\n",
    "EPOCHS = 10\n",
    "\n",
    "for epoch in range(EPOCHS):\n",
    "  # Reset the metrics at the start of the new epoch\n",
    "    train_loss.reset_states()\n",
    "    train_accuracy.reset_states()\n",
    "    test_loss.reset_states()\n",
    "    test_accuracy.reset_states()\n",
    "    \n",
    "#   We have already defined the batch size for this dataset\n",
    "#   Therefore image_batch will have shape (batch_size, num_features)\n",
    "    for image_batch, label_batch in train_ds:\n",
    "        train_step(image_batch, label_batch)\n",
    "\n",
    "    for test_image_batch, test_label_batch in test_ds:\n",
    "        test_step(test_image_batch, test_label_batch)\n",
    "\n",
    "    template = 'Epoch {}, Loss: {:.5f}, Accuracy: {:.2f}, Test Loss: {:.2f}, Test Accuracy: {:.2f}'\n",
    "    print(template.format(epoch+1,\n",
    "                        train_loss.result(),\n",
    "                        train_accuracy.result()*100,\n",
    "                        test_loss.result(),\n",
    "                        test_accuracy.result()*100))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
