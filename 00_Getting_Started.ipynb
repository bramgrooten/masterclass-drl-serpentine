{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div align=\"center\">\n",
    "<a href=\"https://vbti.nl\"><img src=\"https://docs.google.com/uc?export=download&id=1DdCGllL51O5wBuiI0rwygofKx3YIDPHX\" width=\"400\"></a>\n",
    "</div>\n",
    "\n",
    "# Welcome to the DRL masterclass!\n",
    "\n",
    "On 16 and 17 of February we are going to dive into topics of Reinforcement and Deep Reinforcement Learning! During these two days we are going to review a lot of concepts. Since we won't have time to explain everything in detail, we have prepared this notebook to give you a headstart. Please, run this notebook (\"Cell\" -> \"Run All\") and make sure there are no dependencies errors.\n",
    "\n",
    "Since the topic of Deep Learning deserves a separate Masterclass (or two), we are not going to dive into the details of training Deep Learning models. Do not worry, we will try to focus primarily on the Reinforcement Learning part of the Deep Reinforcement Learning and will treat Artificial Neural Networks as tools to approximate functions.\n",
    "\n",
    "In this notebook, we have included an explanation of the main interface and the functionality of  <a href=\"https://gym.openai.com/\">[Open AI Gym]</a>, [Keras](https://keras.io/) and [Tensorflow](https://www.tensorflow.org/). \n",
    "\n",
    "If you are familiar with **OpenAI Gym Interface**, **Gym Wrappers and Monitors**, **Keras Sequential API**, **Gradient Tapes** and **Tensorflow Datasets** you can freely skip this notebook (run it only for the dependencies check). "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Importing Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "pygame 2.0.0 (SDL 2.0.12, python 3.8.5)\n",
      "Hello from the pygame community. https://www.pygame.org/contribute.html\n"
     ]
    }
   ],
   "source": [
    "import gym\n",
    "import matplotlib.pyplot as plt\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import yaml\n",
    "import pygame\n",
    "import seaborn as sns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# OpenAI Gym\n",
    "### Environment Class\n",
    "The main OpenAI Gym class. It encapsulates an environment with arbitrary behind-the-scenes dynamics. You can manually create an Atari environment using this class.\n",
    "\n",
    "You can manualy create an Atari environment like this:\n",
    "```python\n",
    "from gym.envs.atari import AtariEnv\n",
    "env = AtariEnv(game='pong', obs_type='image')\n",
    "```\n",
    "However, there are a lot of premade environments that are stored in a registry. This is convenient because you need to know only the *id* of the environment to create it.\n",
    "\n",
    "```python\n",
    "import gym\n",
    "env = gym.make(id='PongDeterministic-v4')\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can check the list of all available environments with the following command:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Copy-v0', 'RepeatCopy-v0', 'ReversedAddition-v0', 'ReversedAddition3-v0', 'DuplicatedInput-v0', 'Reverse-v0', 'CartPole-v0', 'CartPole-v1', 'MountainCar-v0', 'MountainCarContinuous-v0', 'Pendulum-v0', 'Acrobot-v1', 'LunarLander-v2', 'LunarLanderContinuous-v2', 'BipedalWalker-v3', 'BipedalWalkerHardcore-v3', 'CarRacing-v0', 'Blackjack-v0', 'KellyCoinflip-v0', 'KellyCoinflipGeneralized-v0', 'FrozenLake-v0', 'FrozenLake8x8-v0', 'CliffWalking-v0', 'NChain-v0', 'Roulette-v0', 'Taxi-v3', 'GuessingGame-v0', 'HotterColder-v0', 'Reacher-v2', 'Pusher-v2', 'Thrower-v2', 'Striker-v2', 'InvertedPendulum-v2', 'InvertedDoublePendulum-v2', 'HalfCheetah-v2', 'HalfCheetah-v3', 'Hopper-v2', 'Hopper-v3', 'Swimmer-v2', 'Swimmer-v3', 'Walker2d-v2', 'Walker2d-v3', 'Ant-v2', 'Ant-v3', 'Humanoid-v2', 'Humanoid-v3', 'HumanoidStandup-v2', 'FetchSlide-v1', 'FetchPickAndPlace-v1', 'FetchReach-v1', 'FetchPush-v1', 'HandReach-v0', 'HandManipulateBlockRotateZ-v0', 'HandManipulateBlockRotateZTouchSensors-v0', 'HandManipulateBlockRotateZTouchSensors-v1', 'HandManipulateBlockRotateParallel-v0', 'HandManipulateBlockRotateParallelTouchSensors-v0', 'HandManipulateBlockRotateParallelTouchSensors-v1', 'HandManipulateBlockRotateXYZ-v0', 'HandManipulateBlockRotateXYZTouchSensors-v0', 'HandManipulateBlockRotateXYZTouchSensors-v1', 'HandManipulateBlockFull-v0', 'HandManipulateBlock-v0', 'HandManipulateBlockTouchSensors-v0', 'HandManipulateBlockTouchSensors-v1', 'HandManipulateEggRotate-v0', 'HandManipulateEggRotateTouchSensors-v0', 'HandManipulateEggRotateTouchSensors-v1', 'HandManipulateEggFull-v0', 'HandManipulateEgg-v0', 'HandManipulateEggTouchSensors-v0', 'HandManipulateEggTouchSensors-v1', 'HandManipulatePenRotate-v0', 'HandManipulatePenRotateTouchSensors-v0', 'HandManipulatePenRotateTouchSensors-v1', 'HandManipulatePenFull-v0', 'HandManipulatePen-v0', 'HandManipulatePenTouchSensors-v0', 'HandManipulatePenTouchSensors-v1', 'FetchSlideDense-v1', 'FetchPickAndPlaceDense-v1', 'FetchReachDense-v1', 'FetchPushDense-v1', 'HandReachDense-v0', 'HandManipulateBlockRotateZDense-v0', 'HandManipulateBlockRotateZTouchSensorsDense-v0', 'HandManipulateBlockRotateZTouchSensorsDense-v1', 'HandManipulateBlockRotateParallelDense-v0', 'HandManipulateBlockRotateParallelTouchSensorsDense-v0', 'HandManipulateBlockRotateParallelTouchSensorsDense-v1', 'HandManipulateBlockRotateXYZDense-v0', 'HandManipulateBlockRotateXYZTouchSensorsDense-v0', 'HandManipulateBlockRotateXYZTouchSensorsDense-v1', 'HandManipulateBlockFullDense-v0', 'HandManipulateBlockDense-v0', 'HandManipulateBlockTouchSensorsDense-v0', 'HandManipulateBlockTouchSensorsDense-v1', 'HandManipulateEggRotateDense-v0', 'HandManipulateEggRotateTouchSensorsDense-v0', 'HandManipulateEggRotateTouchSensorsDense-v1', 'HandManipulateEggFullDense-v0', 'HandManipulateEggDense-v0', 'HandManipulateEggTouchSensorsDense-v0', 'HandManipulateEggTouchSensorsDense-v1', 'HandManipulatePenRotateDense-v0', 'HandManipulatePenRotateTouchSensorsDense-v0', 'HandManipulatePenRotateTouchSensorsDense-v1', 'HandManipulatePenFullDense-v0', 'HandManipulatePenDense-v0', 'HandManipulatePenTouchSensorsDense-v0', 'HandManipulatePenTouchSensorsDense-v1', 'Adventure-v0', 'Adventure-v4', 'AdventureDeterministic-v0', 'AdventureDeterministic-v4', 'AdventureNoFrameskip-v0', 'AdventureNoFrameskip-v4', 'Adventure-ram-v0', 'Adventure-ram-v4', 'Adventure-ramDeterministic-v0', 'Adventure-ramDeterministic-v4', 'Adventure-ramNoFrameskip-v0', 'Adventure-ramNoFrameskip-v4', 'AirRaid-v0', 'AirRaid-v4', 'AirRaidDeterministic-v0', 'AirRaidDeterministic-v4', 'AirRaidNoFrameskip-v0', 'AirRaidNoFrameskip-v4', 'AirRaid-ram-v0', 'AirRaid-ram-v4', 'AirRaid-ramDeterministic-v0', 'AirRaid-ramDeterministic-v4', 'AirRaid-ramNoFrameskip-v0', 'AirRaid-ramNoFrameskip-v4', 'Alien-v0', 'Alien-v4', 'AlienDeterministic-v0', 'AlienDeterministic-v4', 'AlienNoFrameskip-v0', 'AlienNoFrameskip-v4', 'Alien-ram-v0', 'Alien-ram-v4', 'Alien-ramDeterministic-v0', 'Alien-ramDeterministic-v4', 'Alien-ramNoFrameskip-v0', 'Alien-ramNoFrameskip-v4', 'Amidar-v0', 'Amidar-v4', 'AmidarDeterministic-v0', 'AmidarDeterministic-v4', 'AmidarNoFrameskip-v0', 'AmidarNoFrameskip-v4', 'Amidar-ram-v0', 'Amidar-ram-v4', 'Amidar-ramDeterministic-v0', 'Amidar-ramDeterministic-v4', 'Amidar-ramNoFrameskip-v0', 'Amidar-ramNoFrameskip-v4', 'Assault-v0', 'Assault-v4', 'AssaultDeterministic-v0', 'AssaultDeterministic-v4', 'AssaultNoFrameskip-v0', 'AssaultNoFrameskip-v4', 'Assault-ram-v0', 'Assault-ram-v4', 'Assault-ramDeterministic-v0', 'Assault-ramDeterministic-v4', 'Assault-ramNoFrameskip-v0', 'Assault-ramNoFrameskip-v4', 'Asterix-v0', 'Asterix-v4', 'AsterixDeterministic-v0', 'AsterixDeterministic-v4', 'AsterixNoFrameskip-v0', 'AsterixNoFrameskip-v4', 'Asterix-ram-v0', 'Asterix-ram-v4', 'Asterix-ramDeterministic-v0', 'Asterix-ramDeterministic-v4', 'Asterix-ramNoFrameskip-v0', 'Asterix-ramNoFrameskip-v4', 'Asteroids-v0', 'Asteroids-v4', 'AsteroidsDeterministic-v0', 'AsteroidsDeterministic-v4', 'AsteroidsNoFrameskip-v0', 'AsteroidsNoFrameskip-v4', 'Asteroids-ram-v0', 'Asteroids-ram-v4', 'Asteroids-ramDeterministic-v0', 'Asteroids-ramDeterministic-v4', 'Asteroids-ramNoFrameskip-v0', 'Asteroids-ramNoFrameskip-v4', 'Atlantis-v0', 'Atlantis-v4', 'AtlantisDeterministic-v0', 'AtlantisDeterministic-v4', 'AtlantisNoFrameskip-v0', 'AtlantisNoFrameskip-v4', 'Atlantis-ram-v0', 'Atlantis-ram-v4', 'Atlantis-ramDeterministic-v0', 'Atlantis-ramDeterministic-v4', 'Atlantis-ramNoFrameskip-v0', 'Atlantis-ramNoFrameskip-v4', 'BankHeist-v0', 'BankHeist-v4', 'BankHeistDeterministic-v0', 'BankHeistDeterministic-v4', 'BankHeistNoFrameskip-v0', 'BankHeistNoFrameskip-v4', 'BankHeist-ram-v0', 'BankHeist-ram-v4', 'BankHeist-ramDeterministic-v0', 'BankHeist-ramDeterministic-v4', 'BankHeist-ramNoFrameskip-v0', 'BankHeist-ramNoFrameskip-v4', 'BattleZone-v0', 'BattleZone-v4', 'BattleZoneDeterministic-v0', 'BattleZoneDeterministic-v4', 'BattleZoneNoFrameskip-v0', 'BattleZoneNoFrameskip-v4', 'BattleZone-ram-v0', 'BattleZone-ram-v4', 'BattleZone-ramDeterministic-v0', 'BattleZone-ramDeterministic-v4', 'BattleZone-ramNoFrameskip-v0', 'BattleZone-ramNoFrameskip-v4', 'BeamRider-v0', 'BeamRider-v4', 'BeamRiderDeterministic-v0', 'BeamRiderDeterministic-v4', 'BeamRiderNoFrameskip-v0', 'BeamRiderNoFrameskip-v4', 'BeamRider-ram-v0', 'BeamRider-ram-v4', 'BeamRider-ramDeterministic-v0', 'BeamRider-ramDeterministic-v4', 'BeamRider-ramNoFrameskip-v0', 'BeamRider-ramNoFrameskip-v4', 'Berzerk-v0', 'Berzerk-v4', 'BerzerkDeterministic-v0', 'BerzerkDeterministic-v4', 'BerzerkNoFrameskip-v0', 'BerzerkNoFrameskip-v4', 'Berzerk-ram-v0', 'Berzerk-ram-v4', 'Berzerk-ramDeterministic-v0', 'Berzerk-ramDeterministic-v4', 'Berzerk-ramNoFrameskip-v0', 'Berzerk-ramNoFrameskip-v4', 'Bowling-v0', 'Bowling-v4', 'BowlingDeterministic-v0', 'BowlingDeterministic-v4', 'BowlingNoFrameskip-v0', 'BowlingNoFrameskip-v4', 'Bowling-ram-v0', 'Bowling-ram-v4', 'Bowling-ramDeterministic-v0', 'Bowling-ramDeterministic-v4', 'Bowling-ramNoFrameskip-v0', 'Bowling-ramNoFrameskip-v4', 'Boxing-v0', 'Boxing-v4', 'BoxingDeterministic-v0', 'BoxingDeterministic-v4', 'BoxingNoFrameskip-v0', 'BoxingNoFrameskip-v4', 'Boxing-ram-v0', 'Boxing-ram-v4', 'Boxing-ramDeterministic-v0', 'Boxing-ramDeterministic-v4', 'Boxing-ramNoFrameskip-v0', 'Boxing-ramNoFrameskip-v4', 'Breakout-v0', 'Breakout-v4', 'BreakoutDeterministic-v0', 'BreakoutDeterministic-v4', 'BreakoutNoFrameskip-v0', 'BreakoutNoFrameskip-v4', 'Breakout-ram-v0', 'Breakout-ram-v4', 'Breakout-ramDeterministic-v0', 'Breakout-ramDeterministic-v4', 'Breakout-ramNoFrameskip-v0', 'Breakout-ramNoFrameskip-v4', 'Carnival-v0', 'Carnival-v4', 'CarnivalDeterministic-v0', 'CarnivalDeterministic-v4', 'CarnivalNoFrameskip-v0', 'CarnivalNoFrameskip-v4', 'Carnival-ram-v0', 'Carnival-ram-v4', 'Carnival-ramDeterministic-v0', 'Carnival-ramDeterministic-v4', 'Carnival-ramNoFrameskip-v0', 'Carnival-ramNoFrameskip-v4', 'Centipede-v0', 'Centipede-v4', 'CentipedeDeterministic-v0', 'CentipedeDeterministic-v4', 'CentipedeNoFrameskip-v0', 'CentipedeNoFrameskip-v4', 'Centipede-ram-v0', 'Centipede-ram-v4', 'Centipede-ramDeterministic-v0', 'Centipede-ramDeterministic-v4', 'Centipede-ramNoFrameskip-v0', 'Centipede-ramNoFrameskip-v4', 'ChopperCommand-v0', 'ChopperCommand-v4', 'ChopperCommandDeterministic-v0', 'ChopperCommandDeterministic-v4', 'ChopperCommandNoFrameskip-v0', 'ChopperCommandNoFrameskip-v4', 'ChopperCommand-ram-v0', 'ChopperCommand-ram-v4', 'ChopperCommand-ramDeterministic-v0', 'ChopperCommand-ramDeterministic-v4', 'ChopperCommand-ramNoFrameskip-v0', 'ChopperCommand-ramNoFrameskip-v4', 'CrazyClimber-v0', 'CrazyClimber-v4', 'CrazyClimberDeterministic-v0', 'CrazyClimberDeterministic-v4', 'CrazyClimberNoFrameskip-v0', 'CrazyClimberNoFrameskip-v4', 'CrazyClimber-ram-v0', 'CrazyClimber-ram-v4', 'CrazyClimber-ramDeterministic-v0', 'CrazyClimber-ramDeterministic-v4', 'CrazyClimber-ramNoFrameskip-v0', 'CrazyClimber-ramNoFrameskip-v4', 'Defender-v0', 'Defender-v4', 'DefenderDeterministic-v0', 'DefenderDeterministic-v4', 'DefenderNoFrameskip-v0', 'DefenderNoFrameskip-v4', 'Defender-ram-v0', 'Defender-ram-v4', 'Defender-ramDeterministic-v0', 'Defender-ramDeterministic-v4', 'Defender-ramNoFrameskip-v0', 'Defender-ramNoFrameskip-v4', 'DemonAttack-v0', 'DemonAttack-v4', 'DemonAttackDeterministic-v0', 'DemonAttackDeterministic-v4', 'DemonAttackNoFrameskip-v0', 'DemonAttackNoFrameskip-v4', 'DemonAttack-ram-v0', 'DemonAttack-ram-v4', 'DemonAttack-ramDeterministic-v0', 'DemonAttack-ramDeterministic-v4', 'DemonAttack-ramNoFrameskip-v0', 'DemonAttack-ramNoFrameskip-v4', 'DoubleDunk-v0', 'DoubleDunk-v4', 'DoubleDunkDeterministic-v0', 'DoubleDunkDeterministic-v4', 'DoubleDunkNoFrameskip-v0', 'DoubleDunkNoFrameskip-v4', 'DoubleDunk-ram-v0', 'DoubleDunk-ram-v4', 'DoubleDunk-ramDeterministic-v0', 'DoubleDunk-ramDeterministic-v4', 'DoubleDunk-ramNoFrameskip-v0', 'DoubleDunk-ramNoFrameskip-v4', 'ElevatorAction-v0', 'ElevatorAction-v4', 'ElevatorActionDeterministic-v0', 'ElevatorActionDeterministic-v4', 'ElevatorActionNoFrameskip-v0', 'ElevatorActionNoFrameskip-v4', 'ElevatorAction-ram-v0', 'ElevatorAction-ram-v4', 'ElevatorAction-ramDeterministic-v0', 'ElevatorAction-ramDeterministic-v4', 'ElevatorAction-ramNoFrameskip-v0', 'ElevatorAction-ramNoFrameskip-v4', 'Enduro-v0', 'Enduro-v4', 'EnduroDeterministic-v0', 'EnduroDeterministic-v4', 'EnduroNoFrameskip-v0', 'EnduroNoFrameskip-v4', 'Enduro-ram-v0', 'Enduro-ram-v4', 'Enduro-ramDeterministic-v0', 'Enduro-ramDeterministic-v4', 'Enduro-ramNoFrameskip-v0', 'Enduro-ramNoFrameskip-v4', 'FishingDerby-v0', 'FishingDerby-v4', 'FishingDerbyDeterministic-v0', 'FishingDerbyDeterministic-v4', 'FishingDerbyNoFrameskip-v0', 'FishingDerbyNoFrameskip-v4', 'FishingDerby-ram-v0', 'FishingDerby-ram-v4', 'FishingDerby-ramDeterministic-v0', 'FishingDerby-ramDeterministic-v4', 'FishingDerby-ramNoFrameskip-v0', 'FishingDerby-ramNoFrameskip-v4', 'Freeway-v0', 'Freeway-v4', 'FreewayDeterministic-v0', 'FreewayDeterministic-v4', 'FreewayNoFrameskip-v0', 'FreewayNoFrameskip-v4', 'Freeway-ram-v0', 'Freeway-ram-v4', 'Freeway-ramDeterministic-v0', 'Freeway-ramDeterministic-v4', 'Freeway-ramNoFrameskip-v0', 'Freeway-ramNoFrameskip-v4', 'Frostbite-v0', 'Frostbite-v4', 'FrostbiteDeterministic-v0', 'FrostbiteDeterministic-v4', 'FrostbiteNoFrameskip-v0', 'FrostbiteNoFrameskip-v4', 'Frostbite-ram-v0', 'Frostbite-ram-v4', 'Frostbite-ramDeterministic-v0', 'Frostbite-ramDeterministic-v4', 'Frostbite-ramNoFrameskip-v0', 'Frostbite-ramNoFrameskip-v4', 'Gopher-v0', 'Gopher-v4', 'GopherDeterministic-v0', 'GopherDeterministic-v4', 'GopherNoFrameskip-v0', 'GopherNoFrameskip-v4', 'Gopher-ram-v0', 'Gopher-ram-v4', 'Gopher-ramDeterministic-v0', 'Gopher-ramDeterministic-v4', 'Gopher-ramNoFrameskip-v0', 'Gopher-ramNoFrameskip-v4', 'Gravitar-v0', 'Gravitar-v4', 'GravitarDeterministic-v0', 'GravitarDeterministic-v4', 'GravitarNoFrameskip-v0', 'GravitarNoFrameskip-v4', 'Gravitar-ram-v0', 'Gravitar-ram-v4', 'Gravitar-ramDeterministic-v0', 'Gravitar-ramDeterministic-v4', 'Gravitar-ramNoFrameskip-v0', 'Gravitar-ramNoFrameskip-v4', 'Hero-v0', 'Hero-v4', 'HeroDeterministic-v0', 'HeroDeterministic-v4', 'HeroNoFrameskip-v0', 'HeroNoFrameskip-v4', 'Hero-ram-v0', 'Hero-ram-v4', 'Hero-ramDeterministic-v0', 'Hero-ramDeterministic-v4', 'Hero-ramNoFrameskip-v0', 'Hero-ramNoFrameskip-v4', 'IceHockey-v0', 'IceHockey-v4', 'IceHockeyDeterministic-v0', 'IceHockeyDeterministic-v4', 'IceHockeyNoFrameskip-v0', 'IceHockeyNoFrameskip-v4', 'IceHockey-ram-v0', 'IceHockey-ram-v4', 'IceHockey-ramDeterministic-v0', 'IceHockey-ramDeterministic-v4', 'IceHockey-ramNoFrameskip-v0', 'IceHockey-ramNoFrameskip-v4', 'Jamesbond-v0', 'Jamesbond-v4', 'JamesbondDeterministic-v0', 'JamesbondDeterministic-v4', 'JamesbondNoFrameskip-v0', 'JamesbondNoFrameskip-v4', 'Jamesbond-ram-v0', 'Jamesbond-ram-v4', 'Jamesbond-ramDeterministic-v0', 'Jamesbond-ramDeterministic-v4', 'Jamesbond-ramNoFrameskip-v0', 'Jamesbond-ramNoFrameskip-v4', 'JourneyEscape-v0', 'JourneyEscape-v4', 'JourneyEscapeDeterministic-v0', 'JourneyEscapeDeterministic-v4', 'JourneyEscapeNoFrameskip-v0', 'JourneyEscapeNoFrameskip-v4', 'JourneyEscape-ram-v0', 'JourneyEscape-ram-v4', 'JourneyEscape-ramDeterministic-v0', 'JourneyEscape-ramDeterministic-v4', 'JourneyEscape-ramNoFrameskip-v0', 'JourneyEscape-ramNoFrameskip-v4', 'Kangaroo-v0', 'Kangaroo-v4', 'KangarooDeterministic-v0', 'KangarooDeterministic-v4', 'KangarooNoFrameskip-v0', 'KangarooNoFrameskip-v4', 'Kangaroo-ram-v0', 'Kangaroo-ram-v4', 'Kangaroo-ramDeterministic-v0', 'Kangaroo-ramDeterministic-v4', 'Kangaroo-ramNoFrameskip-v0', 'Kangaroo-ramNoFrameskip-v4', 'Krull-v0', 'Krull-v4', 'KrullDeterministic-v0', 'KrullDeterministic-v4', 'KrullNoFrameskip-v0', 'KrullNoFrameskip-v4', 'Krull-ram-v0', 'Krull-ram-v4', 'Krull-ramDeterministic-v0', 'Krull-ramDeterministic-v4', 'Krull-ramNoFrameskip-v0', 'Krull-ramNoFrameskip-v4', 'KungFuMaster-v0', 'KungFuMaster-v4', 'KungFuMasterDeterministic-v0', 'KungFuMasterDeterministic-v4', 'KungFuMasterNoFrameskip-v0', 'KungFuMasterNoFrameskip-v4', 'KungFuMaster-ram-v0', 'KungFuMaster-ram-v4', 'KungFuMaster-ramDeterministic-v0', 'KungFuMaster-ramDeterministic-v4', 'KungFuMaster-ramNoFrameskip-v0', 'KungFuMaster-ramNoFrameskip-v4', 'MontezumaRevenge-v0', 'MontezumaRevenge-v4', 'MontezumaRevengeDeterministic-v0', 'MontezumaRevengeDeterministic-v4', 'MontezumaRevengeNoFrameskip-v0', 'MontezumaRevengeNoFrameskip-v4', 'MontezumaRevenge-ram-v0', 'MontezumaRevenge-ram-v4', 'MontezumaRevenge-ramDeterministic-v0', 'MontezumaRevenge-ramDeterministic-v4', 'MontezumaRevenge-ramNoFrameskip-v0', 'MontezumaRevenge-ramNoFrameskip-v4', 'MsPacman-v0', 'MsPacman-v4', 'MsPacmanDeterministic-v0', 'MsPacmanDeterministic-v4', 'MsPacmanNoFrameskip-v0', 'MsPacmanNoFrameskip-v4', 'MsPacman-ram-v0', 'MsPacman-ram-v4', 'MsPacman-ramDeterministic-v0', 'MsPacman-ramDeterministic-v4', 'MsPacman-ramNoFrameskip-v0', 'MsPacman-ramNoFrameskip-v4', 'NameThisGame-v0', 'NameThisGame-v4', 'NameThisGameDeterministic-v0', 'NameThisGameDeterministic-v4', 'NameThisGameNoFrameskip-v0', 'NameThisGameNoFrameskip-v4', 'NameThisGame-ram-v0', 'NameThisGame-ram-v4', 'NameThisGame-ramDeterministic-v0', 'NameThisGame-ramDeterministic-v4', 'NameThisGame-ramNoFrameskip-v0', 'NameThisGame-ramNoFrameskip-v4', 'Phoenix-v0', 'Phoenix-v4', 'PhoenixDeterministic-v0', 'PhoenixDeterministic-v4', 'PhoenixNoFrameskip-v0', 'PhoenixNoFrameskip-v4', 'Phoenix-ram-v0', 'Phoenix-ram-v4', 'Phoenix-ramDeterministic-v0', 'Phoenix-ramDeterministic-v4', 'Phoenix-ramNoFrameskip-v0', 'Phoenix-ramNoFrameskip-v4', 'Pitfall-v0', 'Pitfall-v4', 'PitfallDeterministic-v0', 'PitfallDeterministic-v4', 'PitfallNoFrameskip-v0', 'PitfallNoFrameskip-v4', 'Pitfall-ram-v0', 'Pitfall-ram-v4', 'Pitfall-ramDeterministic-v0', 'Pitfall-ramDeterministic-v4', 'Pitfall-ramNoFrameskip-v0', 'Pitfall-ramNoFrameskip-v4', 'Pong-v0', 'Pong-v4', 'PongDeterministic-v0', 'PongDeterministic-v4', 'PongNoFrameskip-v0', 'PongNoFrameskip-v4', 'Pong-ram-v0', 'Pong-ram-v4', 'Pong-ramDeterministic-v0', 'Pong-ramDeterministic-v4', 'Pong-ramNoFrameskip-v0', 'Pong-ramNoFrameskip-v4', 'Pooyan-v0', 'Pooyan-v4', 'PooyanDeterministic-v0', 'PooyanDeterministic-v4', 'PooyanNoFrameskip-v0', 'PooyanNoFrameskip-v4', 'Pooyan-ram-v0', 'Pooyan-ram-v4', 'Pooyan-ramDeterministic-v0', 'Pooyan-ramDeterministic-v4', 'Pooyan-ramNoFrameskip-v0', 'Pooyan-ramNoFrameskip-v4', 'PrivateEye-v0', 'PrivateEye-v4', 'PrivateEyeDeterministic-v0', 'PrivateEyeDeterministic-v4', 'PrivateEyeNoFrameskip-v0', 'PrivateEyeNoFrameskip-v4', 'PrivateEye-ram-v0', 'PrivateEye-ram-v4', 'PrivateEye-ramDeterministic-v0', 'PrivateEye-ramDeterministic-v4', 'PrivateEye-ramNoFrameskip-v0', 'PrivateEye-ramNoFrameskip-v4', 'Qbert-v0', 'Qbert-v4', 'QbertDeterministic-v0', 'QbertDeterministic-v4', 'QbertNoFrameskip-v0', 'QbertNoFrameskip-v4', 'Qbert-ram-v0', 'Qbert-ram-v4', 'Qbert-ramDeterministic-v0', 'Qbert-ramDeterministic-v4', 'Qbert-ramNoFrameskip-v0', 'Qbert-ramNoFrameskip-v4', 'Riverraid-v0', 'Riverraid-v4', 'RiverraidDeterministic-v0', 'RiverraidDeterministic-v4', 'RiverraidNoFrameskip-v0', 'RiverraidNoFrameskip-v4', 'Riverraid-ram-v0', 'Riverraid-ram-v4', 'Riverraid-ramDeterministic-v0', 'Riverraid-ramDeterministic-v4', 'Riverraid-ramNoFrameskip-v0', 'Riverraid-ramNoFrameskip-v4', 'RoadRunner-v0', 'RoadRunner-v4', 'RoadRunnerDeterministic-v0', 'RoadRunnerDeterministic-v4', 'RoadRunnerNoFrameskip-v0', 'RoadRunnerNoFrameskip-v4', 'RoadRunner-ram-v0', 'RoadRunner-ram-v4', 'RoadRunner-ramDeterministic-v0', 'RoadRunner-ramDeterministic-v4', 'RoadRunner-ramNoFrameskip-v0', 'RoadRunner-ramNoFrameskip-v4', 'Robotank-v0', 'Robotank-v4', 'RobotankDeterministic-v0', 'RobotankDeterministic-v4', 'RobotankNoFrameskip-v0', 'RobotankNoFrameskip-v4', 'Robotank-ram-v0', 'Robotank-ram-v4', 'Robotank-ramDeterministic-v0', 'Robotank-ramDeterministic-v4', 'Robotank-ramNoFrameskip-v0', 'Robotank-ramNoFrameskip-v4', 'Seaquest-v0', 'Seaquest-v4', 'SeaquestDeterministic-v0', 'SeaquestDeterministic-v4', 'SeaquestNoFrameskip-v0', 'SeaquestNoFrameskip-v4', 'Seaquest-ram-v0', 'Seaquest-ram-v4', 'Seaquest-ramDeterministic-v0', 'Seaquest-ramDeterministic-v4', 'Seaquest-ramNoFrameskip-v0', 'Seaquest-ramNoFrameskip-v4', 'Skiing-v0', 'Skiing-v4', 'SkiingDeterministic-v0', 'SkiingDeterministic-v4', 'SkiingNoFrameskip-v0', 'SkiingNoFrameskip-v4', 'Skiing-ram-v0', 'Skiing-ram-v4', 'Skiing-ramDeterministic-v0', 'Skiing-ramDeterministic-v4', 'Skiing-ramNoFrameskip-v0', 'Skiing-ramNoFrameskip-v4', 'Solaris-v0', 'Solaris-v4', 'SolarisDeterministic-v0', 'SolarisDeterministic-v4', 'SolarisNoFrameskip-v0', 'SolarisNoFrameskip-v4', 'Solaris-ram-v0', 'Solaris-ram-v4', 'Solaris-ramDeterministic-v0', 'Solaris-ramDeterministic-v4', 'Solaris-ramNoFrameskip-v0', 'Solaris-ramNoFrameskip-v4', 'SpaceInvaders-v0', 'SpaceInvaders-v4', 'SpaceInvadersDeterministic-v0', 'SpaceInvadersDeterministic-v4', 'SpaceInvadersNoFrameskip-v0', 'SpaceInvadersNoFrameskip-v4', 'SpaceInvaders-ram-v0', 'SpaceInvaders-ram-v4', 'SpaceInvaders-ramDeterministic-v0', 'SpaceInvaders-ramDeterministic-v4', 'SpaceInvaders-ramNoFrameskip-v0', 'SpaceInvaders-ramNoFrameskip-v4', 'StarGunner-v0', 'StarGunner-v4', 'StarGunnerDeterministic-v0', 'StarGunnerDeterministic-v4', 'StarGunnerNoFrameskip-v0', 'StarGunnerNoFrameskip-v4', 'StarGunner-ram-v0', 'StarGunner-ram-v4', 'StarGunner-ramDeterministic-v0', 'StarGunner-ramDeterministic-v4', 'StarGunner-ramNoFrameskip-v0', 'StarGunner-ramNoFrameskip-v4', 'Tennis-v0', 'Tennis-v4', 'TennisDeterministic-v0', 'TennisDeterministic-v4', 'TennisNoFrameskip-v0', 'TennisNoFrameskip-v4', 'Tennis-ram-v0', 'Tennis-ram-v4', 'Tennis-ramDeterministic-v0', 'Tennis-ramDeterministic-v4', 'Tennis-ramNoFrameskip-v0', 'Tennis-ramNoFrameskip-v4', 'TimePilot-v0', 'TimePilot-v4', 'TimePilotDeterministic-v0', 'TimePilotDeterministic-v4', 'TimePilotNoFrameskip-v0', 'TimePilotNoFrameskip-v4', 'TimePilot-ram-v0', 'TimePilot-ram-v4', 'TimePilot-ramDeterministic-v0', 'TimePilot-ramDeterministic-v4', 'TimePilot-ramNoFrameskip-v0', 'TimePilot-ramNoFrameskip-v4', 'Tutankham-v0', 'Tutankham-v4', 'TutankhamDeterministic-v0', 'TutankhamDeterministic-v4', 'TutankhamNoFrameskip-v0', 'TutankhamNoFrameskip-v4', 'Tutankham-ram-v0', 'Tutankham-ram-v4', 'Tutankham-ramDeterministic-v0', 'Tutankham-ramDeterministic-v4', 'Tutankham-ramNoFrameskip-v0', 'Tutankham-ramNoFrameskip-v4', 'UpNDown-v0', 'UpNDown-v4', 'UpNDownDeterministic-v0', 'UpNDownDeterministic-v4', 'UpNDownNoFrameskip-v0', 'UpNDownNoFrameskip-v4', 'UpNDown-ram-v0', 'UpNDown-ram-v4', 'UpNDown-ramDeterministic-v0', 'UpNDown-ramDeterministic-v4', 'UpNDown-ramNoFrameskip-v0', 'UpNDown-ramNoFrameskip-v4', 'Venture-v0', 'Venture-v4', 'VentureDeterministic-v0', 'VentureDeterministic-v4', 'VentureNoFrameskip-v0', 'VentureNoFrameskip-v4', 'Venture-ram-v0', 'Venture-ram-v4', 'Venture-ramDeterministic-v0', 'Venture-ramDeterministic-v4', 'Venture-ramNoFrameskip-v0', 'Venture-ramNoFrameskip-v4', 'VideoPinball-v0', 'VideoPinball-v4', 'VideoPinballDeterministic-v0', 'VideoPinballDeterministic-v4', 'VideoPinballNoFrameskip-v0', 'VideoPinballNoFrameskip-v4', 'VideoPinball-ram-v0', 'VideoPinball-ram-v4', 'VideoPinball-ramDeterministic-v0', 'VideoPinball-ramDeterministic-v4', 'VideoPinball-ramNoFrameskip-v0', 'VideoPinball-ramNoFrameskip-v4', 'WizardOfWor-v0', 'WizardOfWor-v4', 'WizardOfWorDeterministic-v0', 'WizardOfWorDeterministic-v4', 'WizardOfWorNoFrameskip-v0', 'WizardOfWorNoFrameskip-v4', 'WizardOfWor-ram-v0', 'WizardOfWor-ram-v4', 'WizardOfWor-ramDeterministic-v0', 'WizardOfWor-ramDeterministic-v4', 'WizardOfWor-ramNoFrameskip-v0', 'WizardOfWor-ramNoFrameskip-v4', 'YarsRevenge-v0', 'YarsRevenge-v4', 'YarsRevengeDeterministic-v0', 'YarsRevengeDeterministic-v4', 'YarsRevengeNoFrameskip-v0', 'YarsRevengeNoFrameskip-v4', 'YarsRevenge-ram-v0', 'YarsRevenge-ram-v4', 'YarsRevenge-ramDeterministic-v0', 'YarsRevenge-ramDeterministic-v4', 'YarsRevenge-ramNoFrameskip-v0', 'YarsRevenge-ramNoFrameskip-v4', 'Zaxxon-v0', 'Zaxxon-v4', 'ZaxxonDeterministic-v0', 'ZaxxonDeterministic-v4', 'ZaxxonNoFrameskip-v0', 'ZaxxonNoFrameskip-v4', 'Zaxxon-ram-v0', 'Zaxxon-ram-v4', 'Zaxxon-ramDeterministic-v0', 'Zaxxon-ramDeterministic-v4', 'Zaxxon-ramNoFrameskip-v0', 'Zaxxon-ramNoFrameskip-v4', 'CubeCrash-v0', 'CubeCrashSparse-v0', 'CubeCrashScreenBecomesBlack-v0', 'MemorizeDigits-v0']\n"
     ]
    }
   ],
   "source": [
    "from gym import envs\n",
    "all_envs = envs.registry.all()\n",
    "env_ids = [env_spec.id for env_spec in all_envs]\n",
    "print(env_ids)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Interface\n",
    "This class implements three usefull methods:\n",
    "- ```env.reset()``` - resets the state of the environment and returns an initial observation.\n",
    "- ```env.step(action)``` - run one timestep of the environment's dynamics. When end of episode is reached, you are responsible for calling ```reset()``` to reset this environment's state.\n",
    "- ```env.render()``` - renders the environment. Usefull if we would like to observe the agent playing.\n",
    "- ```env.observation_space``` - observation space of the environment\n",
    "- ```env.action_space``` - action space of the environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Observation space type: <class 'gym.spaces.box.Box'>\n",
      "Action space type: <class 'gym.spaces.discrete.Discrete'>\n"
     ]
    }
   ],
   "source": [
    "import gym\n",
    "\n",
    "env = gym.make('CartPole-v1')\n",
    "print('Observation space type:', type(env.observation_space))\n",
    "print('Action space type:', type(env.action_space))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are two major types of observation and action spaces: **Box** and **Discrete**. You can find these objects under ```gym.spaces.box.Box``` and ```gym.spaces.discrete.Discrete```.\n",
    "\n",
    "CartPole observation and action definition, defined [here](https://github.com/openai/gym/blob/master/gym/envs/classic_control/cartpole.py#L26):\n",
    "```\n",
    "    Observation:\n",
    "        Type: Box(4)\n",
    "        Num     Observation               Min                     Max\n",
    "        0       Cart Position             -4.8                    4.8\n",
    "        1       Cart Velocity             -Inf                    Inf\n",
    "        2       Pole Angle                -0.418 rad (-24 deg)    0.418 rad (24 deg)\n",
    "        3       Pole Angular Velocity     -Inf                    Inf\n",
    "    Actions:\n",
    "        Type: Discrete(2)\n",
    "        Num   Action\n",
    "        0     Push cart to the left\n",
    "        1     Push cart to the right\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Box Space\n",
    "**Box** space is continuous space. You can better think about it as a numpy array. It has a shape and for each element the lowest and highest possible value is defined."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Box space: Box(-3.4028234663852886e+38, 3.4028234663852886e+38, (4,), float32)\n",
      "Observation shape: (4,)\n",
      "Low: [-4.8000002e+00 -3.4028235e+38 -4.1887903e-01 -3.4028235e+38]\n",
      "High: [4.8000002e+00 3.4028235e+38 4.1887903e-01 3.4028235e+38]\n"
     ]
    }
   ],
   "source": [
    "print('Box space:', env.observation_space)\n",
    "print('Observation shape:', env.observation_space.shape)\n",
    "print('Low:', env.observation_space.low)\n",
    "print('High:', env.observation_space.high)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Discrete Space\n",
    "**Discrete** space is ... discrete. It is fully defined by the **number of actions**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Discrete space: Discrete(2)\n",
      "Number of actions: 2\n"
     ]
    }
   ],
   "source": [
    "print('Discrete space:', env.action_space)\n",
    "print('Number of actions:', env.action_space.n)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "These spaces are suitable for both **observation** and **action** spaces. There are more spaces under ```gym.spaces```, but we will not need them in this Masterclass."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ```env.reset()```\n",
    "This method should be called **before every episode**. It will return an initial observation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 0.01438535  0.03736129 -0.02220345 -0.04536134]\n"
     ]
    }
   ],
   "source": [
    "obs = env.reset()\n",
    "print(obs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ```env.step(action)```\n",
    "This method is responsible for controlling the environment. ```action``` parameter should comply with the action space of the environment. When correct action is passed it will return a ```tuple``` with 4 elements: ```next_obs, reward, done, info```.\n",
    "\n",
    "- ```next_obs``` - next observation for our agent. Unless state otherwise it has type ```np.float64```.\n",
    "- ```reward``` - scalar reward\n",
    "- ```done``` - boolean value whether the episode is finished or not. After we receive ```done==True``` we need to call ```env.reset()```. Otherwise, the behaviour of the environment will be *undefined*\n",
    "- ```info``` - dictionary with extra values returned by an environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Next obs: [ 0.01513258  0.23279447 -0.02311067 -0.34496614]\n",
      "Reward: 1.0\n",
      "Done: False\n",
      "Info: {}\n"
     ]
    }
   ],
   "source": [
    "action = np.random.randint(env.action_space.n)\n",
    "next_obs, reward, done, info = env.step(action)\n",
    "print('Next obs:', next_obs)\n",
    "print('Reward:', reward)\n",
    "print('Done:', done)\n",
    "print('Info:', info)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The minimal code for a loop that plays one episode should look something like this:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Steps: 20\n"
     ]
    }
   ],
   "source": [
    "done = False\n",
    "steps = 0\n",
    "env.reset()\n",
    "while not done:\n",
    "    action = env.action_space.sample()                  \n",
    "    observation, reward, done, info = env.step(action)  \n",
    "    steps += 1 \n",
    "\n",
    "print('Steps:', steps)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Atari Environments\n",
    "In the previous examples, the environments' input state was a low dimensional vector. What if we don't have the access to the internal information and can only observe the game screen? Arcade Learning Environment is the library that implements many games from the Atari 2060 and allows us to control them through a convenient Gym interface.\n",
    "\n",
    "Arcade Learning Environment ALE <a href=\"https://arxiv.org/pdf/1207.4708.pdf\"> [paper]</a> was developed in 2013 as an evaluation benchmark for general learning algorithms. Open AI gym <a href=\"https://arxiv.org/abs/1606.01540\">[paper]</a>\n",
    "\n",
    "For the Atari games the environments *ids* are following following naming convention:\n",
    "<img src=\"https://i.ibb.co/K9RLckY/env-ids.png\" width=700>\n",
    "<center>Explanation of the naming convention. <a href=\"https://www.endtoend.ai/envs/gym/atari/\">[source]</a><center>\n",
    "\n",
    "    \n",
    "The Atari emulator generates frames at 60Hz. It would be 'unfair' if an algorithm could update its actions every frame, as a human cannot control a joystick at 60Hz. Therefore, some environments apply the same action to multiple frames. This can be a fixed number of frames or a random number of frames. The figure above explains in detail the naming convention.\n",
    "    \n",
    "Since all Atari games are inherently deterministic the brute search is a viable option for solving these games. Repeat action probability *p* is responsible for injecting stochasticity into the environment by holding the action constant for the next frame. This mechanic was proposed in Revisiting the Arcade Learning Environment <a href=\"https://arxiv.org/abs/1709.06009\">[paper]</a>. By making the environment stochastic we make sure that the agent does not rely on memorization when playing the game.\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Gym Wrappers and Monitors\n",
    "\n",
    "\n",
    "### Wrappers\n",
    "\n",
    "Environment wrapper is a very important tool that allows to extend the functionality of an environment in various ways. It can be used to preprocess observations, modify actions or rewards. Wrappers allows to make all of those changes in a very clean way without altering the functionality of an agent.\n",
    "\n",
    "```python\n",
    "class BasicWrapper(gym.Wrapper):\n",
    "    def __init__(self, env):\n",
    "        super().__init__(env)\n",
    "        self.env = env\n",
    "        \n",
    "    def step(self, action):\n",
    "        next_state, reward, done, info = self.env.step(action)\n",
    "        # modify ...\n",
    "        return next_state, reward, done, info\n",
    "```\n",
    "<img src=\"https://i.ibb.co/ZLfyqxt/env-wrapper.png\">\n",
    "\n",
    "## Monitors\n",
    "Monitors is a convenient way of keeping track of what the environment is doing. The most common use of the Monitor is to record a video of an agent playing. This can be done with a simple:\n",
    "\n",
    "```python\n",
    "import gym\n",
    "from gym import wrappers\n",
    "\n",
    "env = gym.wrappers.Monitor(env, directory='recording')\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Example: Random Agent\n",
    "Lets implement a random agent class that have only one ```compute_action()``` method."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "class RandomAgent:\n",
    "\n",
    "    def __init__(self, n_actions):\n",
    "        self.n_actions = n_actions\n",
    "\n",
    "    def compute_action(self, state):\n",
    "        return np.random.randint(self.n_actions)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You do not need much code to run an episode, but it would be nice to keep track the performance of the agent. Since the reward at every time step is the difference between the scores, summing up all of the rewards would give us the final score of the game."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "import matplotlib.pyplot as plt\n",
    "from IPython.core import display\n",
    "\n",
    "# Define loop for one episode\n",
    "def run_episode(env, agent, max_steps=100_000, render=False, sleep=0.):\n",
    "    is_terminal = False\n",
    "    score = 0\n",
    "    steps = 0\n",
    "    start_timer = time.time()\n",
    "\n",
    "    current_state = env.reset()\n",
    "\n",
    "    if render:\n",
    "        img = plt.imshow(env.render(mode='rgb_array'))  # only call this once\n",
    "\n",
    "    while (not is_terminal) and (steps < max_steps):\n",
    "        # get action from the agent\n",
    "\n",
    "        action = agent.compute_action(current_state)\n",
    "\n",
    "        # advance one step in the environment\n",
    "        next_state, reward, is_terminal, info = env.step(action)\n",
    "\n",
    "        # update score\n",
    "        score += reward\n",
    "\n",
    "        # update current state\n",
    "        current_state = next_state\n",
    "\n",
    "        if render:\n",
    "            # render the frame and pause\n",
    "            img.set_data(env.render(mode='rgb_array'))  # just update the data\n",
    "            display.display(plt.gcf())\n",
    "            display.clear_output(wait=True)\n",
    "            time.sleep(sleep)\n",
    "\n",
    "        steps += 1\n",
    "\n",
    "    total_time = time.time() - start_timer\n",
    "    return {'score': score,\n",
    "            'steps_per_game': steps,\n",
    "            'framerate': steps / (total_time + 1e-6)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import gym\n",
    "env = gym.make('PongDeterministic-v4')\n",
    "agent = RandomAgent(env.action_space.n)\n",
    "run_episode(env, agent, render=True, max_steps=100)\n",
    "env.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As you might notice this takes a long time to render an environment in this way. That is why we have limited the number of steps to 100. Now we are going to use a ```Monitor```."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'score': -19.0, 'steps_per_game': 1025, 'framerate': 418.6335588219715}\n"
     ]
    }
   ],
   "source": [
    "from os.path import join\n",
    "video_folder = join('Experiments', 'random_pong')\n",
    "env = gym.make('PongDeterministic-v4')\n",
    "env = gym.wrappers.Monitor(env, video_folder, force=True, video_callable=lambda eps_id: True)\n",
    "\n",
    "agent = RandomAgent(env.action_space.n)\n",
    "\n",
    "stats = run_episode(env, agent, render=False)\n",
    "\n",
    "# You need to call this method for the video to be saved\n",
    "env.close()\n",
    "\n",
    "print(stats)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We have successfully recorded the gameplay of our RandomAgent. We can load the *last* recoded video with the following code."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<video src=\"Experiments\\random_pong\\openaigym.video.0.19872.video000000.mp4\" controls  >\n",
       "      Your browser does not support the <code>video</code> element.\n",
       "    </video>"
      ],
      "text/plain": [
       "<IPython.core.display.Video object>"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from IPython.display import Video\n",
    "import os\n",
    "\n",
    "# take the last element in the folder with .mp4 extension\n",
    "video_name = [x for x in os.listdir(video_folder) if '.mp4' in x][-1]\n",
    "Video(os.path.join(video_folder, video_name))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can also just navigate to the folder and open it with your regular video player."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The same piece of code works for the Cartpole or any other environment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\ikayn\\anaconda3\\envs\\rl_env\\lib\\site-packages\\pyglet\\image\\codecs\\wic.py:289: UserWarning: [WinError -2147417850] Cannot change thread mode after it is set\n",
      "  warnings.warn(str(err))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'score': 13.0, 'steps_per_game': 13, 'framerate': 22.114659098683923}\n"
     ]
    }
   ],
   "source": [
    "from os.path import join\n",
    "video_folder = join('Experiments', 'random_cartpole')\n",
    "env = gym.make('CartPole-v1')\n",
    "env = gym.wrappers.Monitor(env, video_folder, force=True, video_callable=lambda eps_id: True)\n",
    "\n",
    "agent = RandomAgent(env.action_space.n)\n",
    "\n",
    "stats = run_episode(env, agent, render=False)\n",
    "env.close()\n",
    "\n",
    "print(stats)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<video src=\"Experiments\\random_cartpole\\openaigym.video.1.19872.video000000.mp4\" controls  >\n",
       "      Your browser does not support the <code>video</code> element.\n",
       "    </video>"
      ],
      "text/plain": [
       "<IPython.core.display.Video object>"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from IPython.display import Video\n",
    "import os\n",
    "\n",
    "# take the last element in the folder with .mp4 extension\n",
    "video_name = [x for x in os.listdir(video_folder) if '.mp4' in x][-1]\n",
    "Video(os.path.join(video_folder, video_name))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training a Neural Network\n",
    "Since a big part of training Deep Reinforcement Learning agents concerns training neural networks, we include a small example. In this section we are going to show you how to build and train a simple neural network using keras. We are going to use a \"Hello World\" MNIST dataset.\n",
    "\n",
    "## Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((60000, 28, 28),\n",
       " (60000,),\n",
       " (10000, 28, 28),\n",
       " (10000,),\n",
       " dtype('float64'),\n",
       " dtype('uint8'))"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from tensorflow.keras.datasets import mnist\n",
    "\n",
    "# load data\n",
    "(x_train, y_train), (x_val, y_val) = mnist.load_data()\n",
    "\n",
    "# scale data\n",
    "x_train, x_val = x_train / 255.0, x_val / 255.0\n",
    "\n",
    "# inspect shape and type\n",
    "x_train.shape, y_train.shape, x_val.shape, y_val.shape, x_val.dtype, y_train.dtype"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.image.AxesImage at 0x204ca48a310>"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# plot a sample point\n",
    "plt.imshow(x_train[36000], cmap='binary')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Change X array from shape (60000, 28, 28) ... \n",
      "to shape (60000, 784)\n",
      "Before y_train[0] = 5\n",
      "After y_train[0] = [0. 0. 0. 0. 0. 1. 0. 0. 0. 0.]\n"
     ]
    }
   ],
   "source": [
    "# reformat data in the form (batch_size, input_vector_size)\n",
    "print(\"Change X array from shape {} ... \".format(x_train.shape))\n",
    "x_train = x_train.reshape(x_train.shape[0], x_train.shape[1]*x_train.shape[2])\n",
    "x_val = x_val.reshape(x_val.shape[0], x_val.shape[1]*x_val.shape[2])\n",
    "print(\"to shape {}\".format(x_train.shape))\n",
    "\n",
    "# reformat target vector from categorical label to one-hot-encoding\n",
    "from tensorflow.keras import utils\n",
    "\n",
    "# reformat labels to one-hot-encoded labels\n",
    "print('Before y_train[0] = {}'.format(y_train[0]))\n",
    "y_train = utils.to_categorical(y_train, 10)\n",
    "y_val = utils.to_categorical(y_val, 10)\n",
    "print('After y_train[0] = {}'.format(y_train[0]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model\n",
    "### Sequential API [[Guide]](https://keras.io/guides/sequential_model/)\n",
    "\n",
    "Keras allows you to specify a neural network as a collection of layers. With Sequential API you can stack layers on top of each other. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras import models\n",
    "from tensorflow.keras import layers\n",
    "from tensorflow.keras import optimizers\n",
    "\n",
    "# define model topology\n",
    "model = models.Sequential()\n",
    "model.add(layers.Dense(40, activation='relu', input_shape=(784,)))\n",
    "model.add(layers.Dense(20, activation='relu'))\n",
    "model.add(layers.Dense(10, activation='softmax'))\n",
    "\n",
    "# define model optimization method\n",
    "model.compile(optimizer=optimizers.Adam(lr=0.001), \n",
    "              loss='binary_crossentropy', \n",
    "              metrics=['categorical_accuracy'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training a model\n",
    "#### ```model.fit()```\n",
    "We can train a model by calling ```model.fit()```. This is the easiest way of training a model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20\n",
      "1000/1000 [==============================] - 1s 823us/step - loss: 0.0646 - categorical_accuracy: 0.8883 - val_loss: 0.0373 - val_categorical_accuracy: 0.9372\n",
      "Epoch 2/20\n",
      "1000/1000 [==============================] - 1s 775us/step - loss: 0.0333 - categorical_accuracy: 0.9452 - val_loss: 0.0322 - val_categorical_accuracy: 0.9426\n",
      "Epoch 3/20\n",
      "1000/1000 [==============================] - 1s 735us/step - loss: 0.0268 - categorical_accuracy: 0.9554 - val_loss: 0.0262 - val_categorical_accuracy: 0.9553\n",
      "Epoch 4/20\n",
      "1000/1000 [==============================] - 1s 803us/step - loss: 0.0224 - categorical_accuracy: 0.9622 - val_loss: 0.0209 - val_categorical_accuracy: 0.9637\n",
      "Epoch 5/20\n",
      "1000/1000 [==============================] - 1s 880us/step - loss: 0.0197 - categorical_accuracy: 0.9677 - val_loss: 0.0213 - val_categorical_accuracy: 0.9638\n",
      "Epoch 6/20\n",
      "1000/1000 [==============================] - 1s 745us/step - loss: 0.0174 - categorical_accuracy: 0.9712 - val_loss: 0.0207 - val_categorical_accuracy: 0.9653\n",
      "Epoch 7/20\n",
      "1000/1000 [==============================] - 1s 712us/step - loss: 0.0156 - categorical_accuracy: 0.9743 - val_loss: 0.0193 - val_categorical_accuracy: 0.9684\n",
      "Epoch 8/20\n",
      "1000/1000 [==============================] - 1s 709us/step - loss: 0.0143 - categorical_accuracy: 0.9764 - val_loss: 0.0228 - val_categorical_accuracy: 0.9607\n",
      "Epoch 9/20\n",
      "1000/1000 [==============================] - 1s 774us/step - loss: 0.0133 - categorical_accuracy: 0.9783 - val_loss: 0.0187 - val_categorical_accuracy: 0.9682\n",
      "Epoch 10/20\n",
      "1000/1000 [==============================] - 1s 723us/step - loss: 0.0121 - categorical_accuracy: 0.9801 - val_loss: 0.0175 - val_categorical_accuracy: 0.9707\n",
      "Epoch 11/20\n",
      "1000/1000 [==============================] - 1s 731us/step - loss: 0.0113 - categorical_accuracy: 0.9812 - val_loss: 0.0163 - val_categorical_accuracy: 0.9718\n",
      "Epoch 12/20\n",
      "1000/1000 [==============================] - 1s 701us/step - loss: 0.0107 - categorical_accuracy: 0.9820 - val_loss: 0.0183 - val_categorical_accuracy: 0.9705\n",
      "Epoch 13/20\n",
      "1000/1000 [==============================] - 1s 650us/step - loss: 0.0096 - categorical_accuracy: 0.9841 - val_loss: 0.0173 - val_categorical_accuracy: 0.9716\n",
      "Epoch 14/20\n",
      "1000/1000 [==============================] - 1s 740us/step - loss: 0.0092 - categorical_accuracy: 0.9848 - val_loss: 0.0187 - val_categorical_accuracy: 0.9700\n",
      "Epoch 15/20\n",
      "1000/1000 [==============================] - 1s 652us/step - loss: 0.0085 - categorical_accuracy: 0.9857 - val_loss: 0.0185 - val_categorical_accuracy: 0.9724\n",
      "Epoch 16/20\n",
      "1000/1000 [==============================] - 1s 649us/step - loss: 0.0080 - categorical_accuracy: 0.9866 - val_loss: 0.0180 - val_categorical_accuracy: 0.9720\n",
      "Epoch 17/20\n",
      "1000/1000 [==============================] - 1s 644us/step - loss: 0.0076 - categorical_accuracy: 0.9876 - val_loss: 0.0192 - val_categorical_accuracy: 0.9700\n",
      "Epoch 18/20\n",
      "1000/1000 [==============================] - 1s 635us/step - loss: 0.0071 - categorical_accuracy: 0.9883 - val_loss: 0.0194 - val_categorical_accuracy: 0.9707\n",
      "Epoch 19/20\n",
      "1000/1000 [==============================] - 1s 638us/step - loss: 0.0068 - categorical_accuracy: 0.9887 - val_loss: 0.0191 - val_categorical_accuracy: 0.9716\n",
      "Epoch 20/20\n",
      "1000/1000 [==============================] - 1s 633us/step - loss: 0.0064 - categorical_accuracy: 0.9893 - val_loss: 0.0200 - val_categorical_accuracy: 0.9716\n"
     ]
    }
   ],
   "source": [
    "history = model.fit(x_train, y_train, epochs=20, batch_size=60, validation_data=(x_val, y_val))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Functional API [[Guide]](https://keras.io/guides/functional_api/)\n",
    "In case your model needs to have multiple inputs/outputs or the flow of the data is not sequential, we can use ```Functional``` API for this. It allows building more complex models by providing tools for building computational graphs manually. Starting with the input layer ```input_layer = Input()``` which is our input Tensor, we can apply a layer on it and receive the output tensor. \n",
    "\n",
    "```python\n",
    "input_tensor = Input()\n",
    "layer = Dense(4)\n",
    "output_tensor_hidden_1 = layer(input_tensor) \n",
    "```\n",
    "\n",
    "Repeat these operations with other layers to connect all model components. We can also combine outputs and freely reuse the layers if necessary."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20\n",
      "750/750 [==============================] - 1s 2ms/step - loss: 0.0767 - acc: 0.4334 - val_loss: 0.0637 - val_acc: 0.6167\n",
      "Epoch 2/20\n",
      "750/750 [==============================] - 1s 1ms/step - loss: 0.0565 - acc: 0.6644 - val_loss: 0.0495 - val_acc: 0.7147\n",
      "Epoch 3/20\n",
      "750/750 [==============================] - 1s 2ms/step - loss: 0.0457 - acc: 0.7282 - val_loss: 0.0408 - val_acc: 0.7672\n",
      "Epoch 4/20\n",
      "750/750 [==============================] - 1s 1ms/step - loss: 0.0383 - acc: 0.7728 - val_loss: 0.0341 - val_acc: 0.7997\n",
      "Epoch 5/20\n",
      "750/750 [==============================] - 1s 1ms/step - loss: 0.0327 - acc: 0.8026 - val_loss: 0.0292 - val_acc: 0.8334\n",
      "Epoch 6/20\n",
      "750/750 [==============================] - 1s 1ms/step - loss: 0.0284 - acc: 0.8392 - val_loss: 0.0253 - val_acc: 0.8642\n",
      "Epoch 7/20\n",
      "750/750 [==============================] - 1s 2ms/step - loss: 0.0249 - acc: 0.8620 - val_loss: 0.0222 - val_acc: 0.8786\n",
      "Epoch 8/20\n",
      "750/750 [==============================] - 1s 1ms/step - loss: 0.0224 - acc: 0.8739 - val_loss: 0.0201 - val_acc: 0.8869\n",
      "Epoch 9/20\n",
      "750/750 [==============================] - 1s 1ms/step - loss: 0.0205 - acc: 0.8816 - val_loss: 0.0186 - val_acc: 0.8922\n",
      "Epoch 10/20\n",
      "750/750 [==============================] - 1s 1ms/step - loss: 0.0192 - acc: 0.8873 - val_loss: 0.0176 - val_acc: 0.8956\n",
      "Epoch 11/20\n",
      "750/750 [==============================] - 1s 1ms/step - loss: 0.0181 - acc: 0.8921 - val_loss: 0.0167 - val_acc: 0.8991\n",
      "Epoch 12/20\n",
      "750/750 [==============================] - 1s 1ms/step - loss: 0.0173 - acc: 0.8966 - val_loss: 0.0160 - val_acc: 0.9026\n",
      "Epoch 13/20\n",
      "750/750 [==============================] - 1s 1ms/step - loss: 0.0165 - acc: 0.9000 - val_loss: 0.0154 - val_acc: 0.9052\n",
      "Epoch 14/20\n",
      "750/750 [==============================] - 1s 1ms/step - loss: 0.0159 - acc: 0.9039 - val_loss: 0.0150 - val_acc: 0.9068\n",
      "Epoch 15/20\n",
      "750/750 [==============================] - 1s 1ms/step - loss: 0.0154 - acc: 0.9069 - val_loss: 0.0145 - val_acc: 0.9090\n",
      "Epoch 16/20\n",
      "750/750 [==============================] - 1s 1ms/step - loss: 0.0149 - acc: 0.9092 - val_loss: 0.0141 - val_acc: 0.9115\n",
      "Epoch 17/20\n",
      "750/750 [==============================] - 1s 1ms/step - loss: 0.0144 - acc: 0.9118 - val_loss: 0.0137 - val_acc: 0.9145\n",
      "Epoch 18/20\n",
      "750/750 [==============================] - 1s 2ms/step - loss: 0.0141 - acc: 0.9137 - val_loss: 0.0134 - val_acc: 0.9161\n",
      "Epoch 19/20\n",
      "750/750 [==============================] - 1s 1ms/step - loss: 0.0137 - acc: 0.9159 - val_loss: 0.0131 - val_acc: 0.9174\n",
      "Epoch 20/20\n",
      "750/750 [==============================] - 1s 1ms/step - loss: 0.0134 - acc: 0.9180 - val_loss: 0.0129 - val_acc: 0.9187\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.layers import Input, Dense\n",
    "\n",
    "input_layer = Input(shape=(x_train.shape[-1], ))\n",
    "output_hidden_1 = Dense(40, name='hidden_1', activation='tanh')(input_layer)\n",
    "output_hidden_1 = Dense(20, name='hidden_2', activation='tanh')(input_layer)\n",
    "output_hidden_2 = Dense(10, name='hidden_3', activation='tanh')(output_hidden_1)\n",
    "output = Dense(10, name='output', activation='softmax')(output_hidden_2)\n",
    "model = Model(inputs=[input_layer], outputs=[output])\n",
    "\n",
    "# The rest of the configuration is the same\n",
    "optimizer = tf.keras.optimizers.SGD(\n",
    "                learning_rate=0.01,\n",
    "                momentum=0.9,\n",
    "                nesterov=False,\n",
    "                name='SGD',)\n",
    "\n",
    "# Specify the training configuration (optimizer, loss, metrics)\n",
    "model.compile(optimizer=optimizer, loss='mse', metrics=['acc'])\n",
    "\n",
    "history = model.fit(\n",
    "                x=x_train,\n",
    "                y=y_train,\n",
    "                batch_size=64,\n",
    "                epochs=20,\n",
    "                verbose=1, # 0 = silent, 1 = progress bar, 2 = one line per epoch.\n",
    "                validation_split=0.2, #Float between 0 and 1. Fraction of the training data to be used as validation data.\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Custom training loop\n",
    "For Reinforcement Learning applications it is important to know how to train a model with a custom training loop. It gives you more freedom in defining the computational flow. Make sure that you understand how all of these elements work, since we are going to use them for training our DRL agents.\n",
    "\n",
    "### ```Dataset```\n",
    "Important part of the custom training loop is the ```Dataset``` object. It allows to create, shuffle, batch and then iterate through the data in a very straightforward manner: using Python loop syntax `for inputs, targets in dataset:`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a dataset instance, shuffle it and set the batch size\n",
    "train_ds = tf.data.Dataset.from_tensor_slices(\n",
    "    (x_train, y_train)).shuffle(1).batch(60)\n",
    "\n",
    "# The same goes for the testing dataset\n",
    "test_ds = tf.data.Dataset.from_tensor_slices((x_val, y_val)).batch(60)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ```Losses``` and ```Metrics```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define the loss object. It can be any function that accepts\n",
    "# loss(y_true, y_predicted) and returns a scalar value\n",
    "loss_object = tf.keras.losses.binary_crossentropy\n",
    "\n",
    "# define metrics for training\n",
    "train_loss = tf.keras.metrics.Mean(name='train_loss')\n",
    "train_accuracy = tf.keras.metrics.BinaryAccuracy(name='train_accuracy')\n",
    "\n",
    "# define metrics for evaluation\n",
    "test_loss = tf.keras.metrics.Mean(name='test_loss')\n",
    "test_accuracy = tf.keras.metrics.BinaryAccuracy(name='test_accuracy')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ```GradientTape```\n",
    "Now we need to define the function were we are going to take the gradients and perform backpropagation.\n",
    "\n",
    "To take the gradients of the loss with respect to any *tensorflow* variables we can use ```tf.GradientTape()```. If you are not familiar with it you can read more [here](https://medium.com/analytics-vidhya/tf-gradienttape-explained-for-keras-users-cc3f06276f22). \n",
    "\n",
    "```GradientTape``` is a **context manager** object. You can find ore information on **context managers** [here](https://medium.com/better-programming/context-managers-in-python-go-beyond-with-open-as-file-85a27e392114).\n",
    "\n",
    "### Optimizers\n",
    "Afterwards, we will use an **optimizer** to perform a gradient descent. More information on **optimizers** can be found [here](https://analyticsindiamag.com/guide-to-optimizers-for-machine-learning/)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define the optimizer that will perform the backprobagation steps\n",
    "optimizer = tf.keras.optimizers.SGD()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ```@tf.function```\n",
    "In order to make the computations more efficient we can pre-compile Python methods as a part of tensorflow computational graph using **```tf.function```** decorator. More info can be found [here](https://www.tensorflow.org/guide/function)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "@tf.function\n",
    "def train_step(images, labels):\n",
    "    with tf.GradientTape() as tape:\n",
    "        predictions = model(images)\n",
    "        loss = loss_object(labels, predictions)\n",
    "    # take the gradients with respect to all the variables\n",
    "    # that were used inside the context manager above\n",
    "    gradients = tape.gradient(loss, model.trainable_variables)\n",
    "    \n",
    "    # pass the gradients and corresponding variables to optimizer\n",
    "    optimizer.apply_gradients(zip(gradients, model.trainable_variables))\n",
    "    \n",
    "    # record loss and accuracy metric\n",
    "    train_loss(loss)\n",
    "    train_accuracy(labels, predictions)\n",
    "\n",
    "# the same goes for the test step\n",
    "@tf.function\n",
    "def test_step(features, labels):\n",
    "    predictions = model(features, training=False)\n",
    "    t_loss = loss_object(labels, predictions)\n",
    "\n",
    "    test_loss(t_loss)\n",
    "    test_accuracy(labels, predictions)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we just need to iterate through our datasets and call ```train_step()``` method."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1, Loss: 0.06416, Accuracy: 97.82, Test Loss: 0.04, Test Accuracy: 98.60\n",
      "Epoch 2, Loss: 0.03246, Accuracy: 98.93, Test Loss: 0.03, Test Accuracy: 98.93\n",
      "Epoch 3, Loss: 0.02551, Accuracy: 99.16, Test Loss: 0.03, Test Accuracy: 99.11\n",
      "Epoch 4, Loss: 0.02145, Accuracy: 99.31, Test Loss: 0.02, Test Accuracy: 99.20\n",
      "Epoch 5, Loss: 0.01867, Accuracy: 99.41, Test Loss: 0.02, Test Accuracy: 99.27\n",
      "Epoch 6, Loss: 0.01658, Accuracy: 99.47, Test Loss: 0.02, Test Accuracy: 99.28\n",
      "Epoch 7, Loss: 0.01493, Accuracy: 99.53, Test Loss: 0.02, Test Accuracy: 99.30\n",
      "Epoch 8, Loss: 0.01365, Accuracy: 99.57, Test Loss: 0.02, Test Accuracy: 99.34\n",
      "Epoch 9, Loss: 0.01251, Accuracy: 99.62, Test Loss: 0.02, Test Accuracy: 99.34\n",
      "Epoch 10, Loss: 0.01164, Accuracy: 99.65, Test Loss: 0.02, Test Accuracy: 99.34\n"
     ]
    }
   ],
   "source": [
    "# define model topology\n",
    "model = models.Sequential()\n",
    "model.add(layers.Dense(40, activation='relu', input_shape=(784,)))\n",
    "model.add(layers.Dense(20, activation='relu'))\n",
    "model.add(layers.Dense(10, activation='softmax'))\n",
    "\n",
    "\n",
    "EPOCHS = 10\n",
    "\n",
    "for epoch in range(EPOCHS):\n",
    "  # Reset the metrics at the start of the new epoch\n",
    "    train_loss.reset_states()\n",
    "    train_accuracy.reset_states()\n",
    "    test_loss.reset_states()\n",
    "    test_accuracy.reset_states()\n",
    "    \n",
    "#   We have already defined the batch size for this dataset\n",
    "#   Therefore image_batch will have shape (batch_size, num_features)\n",
    "    for image_batch, label_batch in train_ds:\n",
    "        train_step(image_batch, label_batch)\n",
    "\n",
    "    for test_image_batch, test_label_batch in test_ds:\n",
    "        test_step(test_image_batch, test_label_batch)\n",
    "\n",
    "    template = 'Epoch {}, Loss: {:.5f}, Accuracy: {:.2f}, Test Loss: {:.2f}, Test Accuracy: {:.2f}'\n",
    "    print(template.format(epoch+1,\n",
    "                        train_loss.result(),\n",
    "                        train_accuracy.result()*100,\n",
    "                        test_loss.result(),\n",
    "                        test_accuracy.result()*100))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
